\chapter{State of the art}
\label{stato dell'arte}


Capitolo uno: 
esempio di bibliografia \cite{2015arXiv150200046K}, un esempio di figura \ref{figure:test inserimento} e un esempio di tabella \ref{table:esempio_tabella}.

\begin{figure}
	\centering
	\includegraphics[width=8cm]{images/example.png}
	\caption{Test inserimento immagini}
	\label{figure:test inserimento}
\end{figure}

\begin{table}[!htbp]
	\centering
	\begin{tabular}{l|l}
		\toprule
		Colonna 1 & Colonna 2 \\
		\midrule
		Valore 1 & Valore 2 \\
		Valore 3 & Valore 4 \\
		Valore 5 & Valore 6 \\
		\bottomrule
	\end{tabular}
	\caption{Descrizione della tabella}
	\label{table:esempio_tabella}
\end{table}


% ------------------------------------------------------------------
\section{Neural Network Computations}\label{s:fundamentals}
% ------------------------------------------------------------------

This chapter provides a brief introduction to the computational aspects of neural networks, and convolutional neural networks in particular, emphasizing the concepts required to understand and use \matconvnet.

% ------------------------------------------------------------------
\subsection{Overview}\label{s:cnn-structure}
% ------------------------------------------------------------------

A \emph{Neural Network} (NN) is a function $g$ mapping data $\bx$, for example an image, to an output vector $\by$, for example an image label. The function $g=f_L \circ \dots \circ f_1$ is the composition of a sequence of simpler functions $f_l$, which are called \emph{computational blocks} or \emph{layers}. Let $\bx_1,\bx_2,\dots,\bx_L$ be the outputs of each layer in the network, and let $\bx_0=\bx$ denote the network input. Each intermediate output $\bx_l = f_l(\bx_{l-1};\bw_l)$ is computed from the previous output $\bx_{l-1}$  by applying the function $f_l$ with parameters $\bw_l$. 

In a \emph{Convolutional Neural Network} (CNN), the data has a spatial structure: each $\bx_l\in\mathbb{R}^{H_l \times W_l \times C_l}$ is a 3D array or \emph{tensor} where the first two dimensions $H_l$ (height) and $W_l$ (width) are interpreted as spatial dimensions. The third dimension $C_l$ is instead interpreted as the \emph{number of feature channels}. Hence, the tensor $\bx_l$ represents a $H_l \times W_l$ field of $C_l$-dimensional feature vectors, one for each spatial location. A fourth dimension $N_l$ in the tensor spans multiple data samples packed in a single \emph{batch} for efficiency parallel processing. The number of data samples $N_l$ in a batch is called the batch \emph{cardinality}. The network is called \emph{convolutional} because the functions $f_l$ are local and translation invariant operators (i.e.\ non-linear filters) like linear convolution.

It is also possible to conceive CNNs with more than two spatial dimensions, where the additional dimensions may represent volume or time. In fact, there are little \emph{a-priori} restrictions on the format of data in neural networks in general. Many useful NNs contain a mixture of convolutional layers together with layer that process other data types such as text strings, or perform other operations that do not strictly conform  to the CNN assumptions.

variety of layers such as (convolution),  (convolution transpose or deconvolution),  (max and average pooling),  (ReLU activation),  (sigmoid activation),  (softmax operator),  (classification log-loss), (batch normalization), (spatial normalization), (local response normalization -- LRN), or ($p$-distance).  There are enough layers to implement many interesting state-of-the-art networks out of the box, or even import them from other toolboxes such as Caffe. 

NNs are often used as classifiers or regressors. In the example of \cref{f:demo}, the output $\hat \by = f(\bx)$ is a vector of probabilities, one for each of a 1,000 possible image labels (dog, cat, trilobite, ...).  If $\by$ is the true label of image $\bx$, we can measure the CNN performance by a loss function $\ell_\by(\hat \by)  \in \mathbb{R}$ which assigns a penalty to classification errors. The CNN parameters can then be tuned or \emph{learned} to minimize this loss averaged over a large dataset of labelled example images.

Learning generally uses a variant of \emph{stochastic gradient descent} (SGD). While this is an efficient method (for this type of problems), networks may contain several million parameters and need to be trained on millions of images. SGD also requires to compute the CNN derivatives, as explained in the next section.

\begin{figure}
	%\centering
	%\includegraphics[width=0.5\columnwidth]{figures/pepper}
	\hrule
	\begin{lstlisting}[escapechar=!]
	% install and compile MatConvNet (run once)
	untar(['http://www.vlfeat.org/matconvnet/download/' ...
	'matconvnet-1.0-beta25.tar.gz']) ;
	cd matconvnet-1.0-beta25
	run matlab/vl_compilenn
	
	% download a pre-trained CNN from the web (run once)
	urlwrite(...
	'http://www.vlfeat.org/matconvnet/models/imagenet-vgg-f.mat', ...
	'imagenet-vgg-f.mat') ;
	
	% setup MatConvNet
	run matlab/vl_setupnn
	
	% load the pre-trained CNN
	net = load('imagenet-vgg-f.mat') ;
	
	% load and preprocess an image
	im = imread('peppers.png') ;
	im_ = imresize(single(im), net.meta.normalization.imageSize(1:2)) ;
	im_ = im_ - net.meta.normalization.averageImage ;
	
	% run the CNN
	res = vl_simplenn(net, im_) ;
	
	% show the classification result
	scores = squeeze(gather(res(end).x)) ;
	[bestScore, best] = max(scores) ;
	figure(1) ; clf ; imagesc(im) ;!
	\begin{tikzpicture}[overlay]
	\node (x) {};
	\node (y) at (7,1) {\includegraphics[width=5cm]{figures/pepper}};
	\draw [->,thick] (.1,.1) -- (4.5,.1) {};
	\end{tikzpicture}!
	title(sprintf('%s (%d), score %.3f',...
	net.classes.description{best}, best, bestScore)) ;
	\end{lstlisting}
	\hrule
	\caption{A complete example including download, installing, compiling and running  \matconvnet to classify one of  \matlab stock images using a large CNN pre-trained on ImageNet.}
	\label{f:demo}
\end{figure}

% ------------------------------------------------------------------
\subsection{Network structures}\label{s:cnn-topology}
% ------------------------------------------------------------------

In the simplest case, layers in a NN are arranged in a sequence; however, more complex interconnections are possible as well, and in fact very useful in many cases. This section discusses such configurations and introduces a graphical notation to visualize them.

% ------------------------------------------------------------------
\subsubsection{Sequences}\label{s:cnn-simple}
% ------------------------------------------------------------------

Start by considering a computational block $f$ in the network. This can be represented schematically as a box receiving data $\bx$ and parameters $\bw$ as inputs and producing data $\by$ as output:
\begin{center}
	\begin{tikzpicture}[auto, node distance=2cm]
	\node (x) [data] {$\bx$};
	\node (f) [block,right of=x]{$f$};
	\node (y) [data, right of=f] {$\by$};
	\node (w) [data, below of=f] {$\bw$};
	\draw [to] (x.east) -- (f.west) {};
	\draw [to] (f.east) -- (y.west) {};
	\draw [to] (w.north) -- (f.south) {};
	\end{tikzpicture}
\end{center}
As seen above, in the simplest case blocks are chained in a sequence $f_1 \rightarrow f_2\rightarrow\dots\rightarrow f_L$ yielding the structure:
\begin{center}
	\begin{tikzpicture}[auto, node distance=2cm]
	\node (x0)  [data] {$\bx_0$};
	\node (f1) [block,right of=x0]{$f_1$};
	\node (f2) [block,right of=f1,node distance=3cm]{$f_2$};
	\node (dots) [right of=f2]{...};
	\node (fL) [block,right of=dots]{$f_L$};
	\node (xL)  [data, right of=fL] {$\bx_L$};
	\node (w1) [data, below of=f1] {$\bw_1$};
	\node (w2) [data, below of=f2] {$\bw_2$};
	\node (wL) [data, below of=fL] {$\bw_L$};
	\draw [to] (x0.east) -- (f1.west) {};
	\draw [to] (f1.east) -- node {$\bx_1$} (f2.west);
	\draw [to] (f2.east) -- node {$\bx_2$} (dots.west) {};
	\draw [to] (dots.east) -- node {$\bx_{L-1}$} (fL.west) {};
	\draw [to] (fL.east) -- (xL.west) {};
	\draw [to] (w1.north) -- (f1.south) {};
	\draw [to] (w2.north) -- (f2.south) {};
	\draw [to] (wL.north) -- (fL.south) {};
	\end{tikzpicture}
\end{center}
Given an input $\bx_0$, evaluating the network is a simple matter of evaluating all the blocks from left to right, which defines a composite function $\bx_L = f(\bx_0;\bw_1,\dots,\bw_L)$. 

% ------------------------------------------------------------------
\subsubsection{Directed acyclic graphs}\label{s:cnn-dag}
% ------------------------------------------------------------------

\begin{figure}[t]
	\begin{center}
		\begin{tikzpicture}[auto, node distance=0.4cm]
		\matrix (m) [matrix of math nodes, 
		column sep=1.2cm,
		row sep=0.4cm]
		{
			& \node (f1) [block]{f_1}; 
			& \node (x1) [datac]{\bx_1};
			\\
			\node (x0) [datac]{\bx_0};
			&
			&
			& \node (f3) [block]{f_3};
			& \node (x3) [datac]{\bx_3};
			\\
			& \node (f2) [block]{f_2}; 
			& \node (x2) [datac]{\bx_2};
			& &
			& \node (f5) [block]{f_5}; 
			& \node (x7) [datac]{\bx_7}; 
			\\
			& 
			& \node(x5) [datac]{\bx_5};
			\\
			\node (x4) [datac]{\bx_4};
			& \node (f4) [block]{f_4};
			\\
			& 
			& \node(x6) [datac]{\bx_6};
			\\
		};
		\draw[to] (x0) -- (f1);
		\draw[to] (f1) -- (x1);
		\draw[to] (x1) -- (f3);
		\draw[to] (x0) -- (f2);
		\draw[to] (f2) -- (x2);
		\draw[to] (x2) -- (f3);
		\draw[to] (f3) -- (x3);
		\draw[to] (x3) -- (f5);
		\draw[to] (f5) -- (x7);
		\draw[to] (x4) -- (f4);
		\draw[to] (f4) -- (x5);
		\draw[to] (f4) -- (x6);
		\draw[to] (x5) -- (f5);
		\node(w1) [par,below=of f1]{$\bw_1$}; \draw[to] (w1) -- (f1);
		\node(w2) [par,below=of f2]{$\bw_2$}; \draw[to] (w2) -- (f2);
		%\node(w3) [par,below=of f3]{$\bw_3$}; \draw[to] (w3) -- (f3);
		\node(w4) [par,below=of f4]{$\bw_4$}; \draw[to] (w4) -- (f4);
		\draw[to] (w4) to [bend right] (f3);
		\node(w5) [par,below=of f5]{$\bw_5$}; \draw[to] (w5) -- (f5);
		\end{tikzpicture}
	\end{center}
	\vspace{-1em}
	\caption{\textbf{Example DAG.}}\label{f:dag}
\end{figure}

One is not limited to chaining layers one after another. In fact, the only requirement for evaluating a NN is that, when a layer has to be evaluated, all its input have been evaluated prior to it. This is possible exactly when the interconnections between layers form a \emph{directed acyclic graph}, or DAG for short.

In order to visualize DAGs, it is useful to introduce additional nodes for the network variables, as in the  example of Fig.~\ref{f:dag}. Here boxes denote functions and circles denote variables (parameters are treated as a special kind of variables). In the example, $\bx_0$ and $\bx_4$ are the inputs of the CNN and $\bx_6$ and $\bx_7$ the outputs. Functions can take any number of inputs (e.g. $f_3$ and $f_5$ take two) and have any number of outputs (e.g. $f_4$ has two). There are a few noteworthy properties of this graph:

\begin{enumerate}
	\item The graph is bipartite, in the sense that arrows always go from boxes to circles and from circles to boxes. 
	\item Functions can have any number of inputs or outputs; variables and parameters can have an arbitrary number of outputs (a parameter with more of one output is \emph{shared} between different layers); variables have at most one input and parameters none. 
	\item Variables with no incoming arrows and parameters are not computed by the network, but must be set prior to evaluation, i.e.\ they are \emph{inputs}. Any variable (or even parameter) may be used as output, although these are usually the variables with no outgoing arrows.
	\item Since the graph is acyclic, the CNN can be evaluated by sorting the functions and computing them one after another (in the example, evaluating the functions in the order $f_1,f_2,f_3,f_4,f_5$ would work).
\end{enumerate}

% ------------------------------------------------------------------
\subsection{Computing derivatives with backpropagation}\label{s:back}
% ------------------------------------------------------------------

Learning a NN requires computing the derivative of the loss with respect to the network parameters. Derivatives are computed using an algorithm called \emph{backpropagation}, which is a memory-efficient implementation of the chain rule for derivatives. First, we discuss the derivatives of a single layer, and then of a whole network.

\subsubsection{Derivatives of tensor functions}

In a CNN, a layer is a function $\by = f(\bx)$ where both input $\bx \in \mathbb{R}^{H\times W \times C}$ and output $\by \in \mathbb{R}^{H'\times W' \times C'}$ are tensors. The derivative of the function $f$ contains the derivative of each output component $y_{i'j'k'}$ with respect to each input component $x_{ijk}$, for a total of $H'\times W'\times C'\times H\times W\times C$ elements naturally arranged in a 6D tensor. Instead of expressing derivatives as tensors, it is often useful  to switch to a matrix notation by \emph{stacking} the input and output tensors into vectors. This is done by the $\vv$ operator, which visits each element of a tensor in lexicographical order and produces a vector:
\[
\vv \bx
=
\begin{bmatrix}
x_{111} \\
x_{211} \\
\vdots
\\
x_{H11} \\
x_{121} \\
\vdots \\
x_{HWC}  	
\end{bmatrix}.
\]
By stacking both input and output, each layer $f$ can be seen reinterpreted as vector function $\vv f$, whose derivative is the conventional Jacobian matrix:
\[
\renewcommand*{\arraystretch}{1.5}
\frac{d \vv f}{d(\vv \bx)^\top}
=
\begin{bmatrix}
\frac{\partial y_{111}}{\partial x_{111}} & 
\frac{\partial y_{111}}{\partial x_{211}} &
\dots &
\frac{\partial y_{111}}{\partial x_{H11}} &
\frac{\partial y_{111}}{\partial x_{121}} &
\dots &
\frac{\partial y_{111}}{\partial x_{HWC}} \\
\frac{\partial y_{211}}{\partial x_{111}} & 
\frac{\partial y_{211}}{\partial x_{211}} &
\dots &
\frac{\partial y_{211}}{\partial x_{H11}} &
\frac{\partial y_{211}}{\partial x_{121}} &
\dots &
\frac{\partial y_{211}}{\partial x_{HWC}} \\
\vdots & \vdots & \dots & \vdots & \vdots & \dots & \vdots \\
\frac{\partial y_{H'11}}{\partial x_{111}} & 
\frac{\partial y_{H'11}}{\partial x_{211}} &
\dots &
\frac{\partial y_{H'11}}{\partial x_{H11}} &
\frac{\partial y_{H'11}}{\partial x_{121}} &
\dots &
\frac{\partial y_{H'11}}{\partial x_{HWC}} \\
\frac{\partial y_{121}}{\partial x_{111}} & 
\frac{\partial y_{121}}{\partial x_{211}} &
\dots &
\frac{\partial y_{121}}{\partial x_{H11}} &
\frac{\partial y_{121}}{\partial x_{121}} &
\dots &
\frac{\partial y_{121}}{\partial x_{HWC}} \\
\vdots & \vdots & \dots & \vdots & \vdots & \dots & \vdots \\
\frac{\partial y_{H'W'C'}}{\partial x_{111}} & 
\frac{\partial y_{H'W'C'}}{\partial x_{211}} &
\dots &
\frac{\partial y_{H'W'C'}}{\partial x_{H11}} &
\frac{\partial y_{H'W'C'}}{\partial x_{121}} &
\dots &
\frac{\partial y_{H'W'C'}}{\partial x_{HWC}}
\end{bmatrix}.
\]
This notation for the derivatives of tensor functions is taken from~\cite{kinghorn96integrals} and is used throughout this document.

While it is easy to express the derivatives of tensor functions as matrices, these matrices are in general extremely large. Even for moderate data sizes (e.g. $H=H'=W=W'=32$ and $C=C'=128$), there are $H'W'C'HWC \approx 17 \times 10^9$ elements in the Jacobian. Storing that requires 68 GB of space in single precision. The purpose of the backpropagation algorithm is to compute the derivatives required for learning without incurring this huge memory cost.

\subsubsection{Derivatives of function compositions}

In order to understand backpropagation, consider first a simple CNN terminating in a loss function $f_L = \ell_\by$:
\begin{center}
	\begin{tikzpicture}[auto, node distance=2cm]
	\node (x0)  [data] {$\bx_0$};
	\node (f1) [block,right of=x0]{$f_1$};
	\node (f2) [block,right of=f1,node distance=3cm]{$f_2$};
	\node (dots) [right of=f2]{...};
	\node (fL) [block,right of=dots]{$f_L$};
	\node (w1) [data, below of=f1] {$\bw_1$};
	\node (w2) [data, below of=f2] {$\bw_2$};
	\node (wL) [data, below of=fL] {$\bw_L$};
	\node (xL) [data, right of=fL] {$x_L\in\real$};
	\draw [to] (x0.east) -- (f1.west) {};
	\draw [to] (f1.east) -- node {$\bx_1$} (f2.west);
	\draw [to] (f2.east) -- node {$\bx_2$} (dots.west) {};
	\draw [to] (dots.east) -- node {$\bx_{L-1}$} (fL.west) {};
	\draw [to] (fL.east) -- (xL.west) {};
	\draw [to] (w1.north) -- (f1.south) {};
	\draw [to] (w2.north) -- (f2.south) {};
	\draw [to] (wL.north) -- (fL.south) {};
	\end{tikzpicture}
\end{center}
The goal is to compute the gradient of the loss value $x_L$ (output) with respect to each network parameter $\bw_l$:
\[
\frac{df}{d(\vv \bw_l)^\top} = 
\frac{d}{d(\vv \bw_l)^\top}
\left[f_L(\cdot;\bw_L) \circ ... \circ 
f_2(\cdot;\bw_2) \circ f_1(\bx_0;\bw_1)\right].
\]
By applying the chain rule and by using the matrix notation introduced above, the derivative can be written as
\begin{equation}\label{e:chain-rule}
\frac{df}{d(\vv \bw_l)^\top} 
= 
\frac{d\vv f_L(\bx_{L-1};\bw_{L})}{d(\vv\bx_{L-1})^\top}
\times
\dots
\times
\frac{d\vv f_{l+1}(\bx_{l};\bw_{l+1})}{d(\vv\bx_{l})^\top}
\times
\frac{d\vv f_l(\bx_{l-1};\bw_{l})}{d(\vv\bw_l^\top)}
\end{equation}
where the derivatives are computed at the working point determined by the input $\bx_0$ and the current value of the parameters. 

Note that, since the network output $x_L$ is a \emph{scalar} quantity, the target derivative $df/d(\vv \bw_l)^\top$ has the same number of elements of the parameter vector $\bw_l$, which is moderate. However, the intermediate Jacobian factors have, as seen above, an unmanageable size. In order to avoid computing these factor explicitly, we can proceed as follows.

Start by multiplying the output of the last layer by a tensor $p_L=1$ (note that this tensor is a scalar just like the variable $x_L$):
\begin{align*}
p_L \times \frac{df}{d(\vv \bw_l)^\top} 
&= 
\underbrace{p_L \times \frac{d\vv f_L(\bx_{L-1};\bw_{L})}{d(\vv\bx_{L-1})^\top}}_{(\vv \bp_{L-1})^\top}
\times
\dots
\times
\frac{d\vv f_{l+1}(\bx_{l};\bw_{l+1})}{d(\vv\bx_{l})^\top}
\times
\frac{d\vv f_l(\bx_{l-1};\bw_{l})}{d(\vv\bw_l^\top)}
\\
&=
(\vv \bp_{L-1})^\top
\times
\dots
\times
\frac{d\vv f_{l+1}(\bx_{l};\bw_{l+1})}{d(\vv\bx_{l})^\top}
\times
\frac{d\vv f_l(\bx_{l-1};\bw_{l})}{d(\vv\bw_l^\top)}
\end{align*}
In the second line the last two factors to the left have been multiplied obtaining a new tensor $\bp_{L-1}$ that has the same size as the variable $\bx_{L-1}$. The factor $\bp_{L-1}$ can therefore be explicitly stored. The construction is then repeated by multiplying pairs of factors from left to right, obtaining a sequence of tensors $\bp_{L-2},\dots,\bp_{l}$ until the desired derivative is obtained. Note that, in doing so, no large tensor is ever stored in memory. This process is known as \emph{backpropagation}.

In general, tensor $\bp_{l}$ is obtained from $\bp_{l+1}$ as the product:
\[
(\vv \bp_{l})^\top = (\vv \bp_{l+1})^\top \times \frac{d\vv f_{l+1}(\bx_{l};\bw_{l+1})}{d(\vv\bx_{l})^\top}.
\]
The key to implement backpropagation is to be able to compute these products without explicitly computing and storing in memory the second factor, which is a large Jacobian matrix. Since computing the derivative is a linear operation, this product can be interpreted as the \emph{derivative of the layer projected along direction $\bp_{l+1}$}: 
\begin{equation}\label{e:projected}
\bp_{l} = 
\frac{d \langle \bp_{l+1}, f(\bx_l;\bw_l) \rangle}
{d \bx_{l}}.
\end{equation}
Here $\langle \cdot,\cdot \rangle$ denotes the inner product between tensors, which results in a scalar quantity. Hence the derivative \eqref{e:projected} needs not to use the $\vv$ notation, and yields a tensor $\bp_l$ that has the same size as $\bx_l$ as expected.

In order to implement backpropagation, a CNN toolbox provides implementations of each layer $f$ that provide:
\begin{itemize}
	\item A \textbf{forward mode}, computing the output $\by = f(\bx;\bw)$ of the layer given its input $\bx$ and parameters $\bw$.
	\item A \textbf{backward mode}, computing the projected derivatives
	\[
	\frac{d \langle \bp, f(\bx;\bw) \rangle}
	{d \bx}
	\quad\text{and}\quad
	\frac{d \langle \bp, f(\bx;\bw) \rangle}
	{d \bw},
	\]
	given, in addition to the input $\bx$ and parameters $\bw$, a tensor $\bp$ that the same size as $\by$.
\end{itemize}
This is best illustrated with an example. Consider a layer $f$ such as the convolution operator. In the ``forward'' mode, one calls the function as $y = vl_nnconv(x,w,[])$ to apply the filters $w$ to the input $x$ and obtain the output $y$. In the ``backward mode'', one calls $[dx, dw] = vl_nnconv(x,w,[],p)$.  As explained above, $dx$, $dw$, and $p$ have the same size as $x$, $w$, and $y$, respectively. The computation of large Jacobian is encapsulated in the function call and never carried out explicitly. 

\subsubsection{Backpropagation networks}\label{s:bpnets}

In this section, we provide a schematic interpretation of backpropagation and show how it can be implemented by ``reversing'' the NN computational graph.

The projected derivative of eq.~\eqref{e:projected} can be seen as the derivative of the following mini-network:
\begin{center}
	\begin{tikzpicture}[auto, node distance=2cm]
	\node (x) [data] {$\bx$};
	\node (f) [block,right of=x ] {$f$};
	\node (dot)[block,right of=f ] {$\langle \cdot, \cdot \rangle$};
	\node (z) [data, right of=dot] {$z \in \mathbb{R}$};
	\node (w) [data, below of=f ] {$\bw$};
	\node (p) [data, below of=dot] {$\bp$};
	\draw [to] (x.east) -- (f.west) {};
	\draw [to] (f.east) -- node {$\by$}  (dot.west) {};
	\draw [to] (w.north) -- (f.south) {};
	\draw [to] (dot.east) -- (z.west) {};
	\draw [to] (p.north) -- (dot.south) {};
	\end{tikzpicture}
\end{center}
In the context of back-propagation, it can be useful to think of the projection $\bp$ as the ``linearization'' of the rest of the network from variable $\by$ down to the loss. The projected derivative can also be though of as a new layer $(d\bx, d\bw) = df(\bx,\bw,\bp)$ that, by computing the derivative of the mini-network, operates in the reverse direction:
\begin{center}
	\begin{tikzpicture}[auto, node distance=2cm]
	\node (df) [block,right of=x] {$df$};
	\node (dx) [data,left of=df] {$d\bx$};
	\node (dw) [data,below of=df] {$d\bw$};
	\node (w) [data,above of=df,xshift=0.6em] {$\bw$};
	\node (x) [data,above of=df,xshift=-0.6em] {$\bx$};
	\node (p) [data,right of=df] {$\bp$};
	\draw [to] (df.west) -- (dx.east)  {};
	\draw [to] (df.south) -- (dw.north)  {};
	\draw [to] (p.west) -- (f.east) {};
	\draw [to] (w.south) -- ([xshift=0.6em]df.north) {};
	\draw [to] (x.south) -- ([xshift=-0.6em]df.north) {};
	\end{tikzpicture}
\end{center}
By construction (see eq.~\eqref{e:projected}), the function $df$ is \emph{linear} in the argument $\bp$.

Using this notation, the forward and backward passes through the original network can be rewritten as evaluating an extended network which contains a BP-reverse of the original one (in blue in the diagram):
\begin{center}
	\begin{tikzpicture}[auto, node distance=2cm]
	\node (x0) [data] {$\bx_0$};
	%
	\node (f1) [block,right of=x0] {$f_1$};
	\node (x1) [data,right of=f1] {$\bx_{1}$};
	\node (w1) [data,below of=f1] {$\bw_1$};
	%
	\node (f2) [block,right of=x1] {$f_2$};
	\node (x2) [data,right of=f2] {$\bx_{2}$};
	\node (w2) [data,below of=f2] {$\bw_2$};
	%
	\node (f3) [right of=x2] {$\dots$};
	\node (xLm) [right of=f3] {$\bx_{L-1}$};
	%
	\node (fL) [block,right of=xLm] {$f_L$};
	\node (xL) [data,right of=fL] {$\bx_{L}$};
	\node (wL) [data,below of=fL] {$\bw_L$};
	%
	\draw [to] (x0.east) -- (f1.west) {};
	%
	\draw [to] (w1.north) -- (f1.south) {};
	\draw [to] (f1.east) -- (x1.west) {};
	\draw [to] (x1.east) -- (f2.west) {};
	%
	\draw [to] (w2.north) -- (f2.south) {};
	\draw [to] (f2.east) -- (x2.west) {};
	\draw [to] (x2.east) -- (f3.west) {};
	%
	\draw [to] (f3.east) -- (xLm.west) {};
	\draw [to] (xLm.east) -- (fL.west) {};
	%
	\draw [to] (wL.north) -- (fL.south) {};
	\draw [to] (fL.east) -- (xL.west) {};
	%
	\node (dfL) [block,below of=wL,bp] {$df_L$};
	\node (dxL) [data,right of=dfL,bpe] {$d\bp_L$};
	\node (dwL) [data,below of=dfL,bpe] {$d\bw_L$};
	\node (dxLm) [data,left of=dfL,bpe] {$d\bx_{L-1}$};
	%
	\node (df3) [left of=dxLm,bpe] {$\dots$};
	%
	\node (df2) [block,below of=w2,bp] {$df_2$};
	\node (dx2) [data,right of=df2,bpe] {$d\bx_{2}$};
	\node (dw2) [data,below of=df2,bpe] {$d\bw_2$};
	%
	\node (df1) [block,below of=w1,bp] {$df_1$};
	\node (dx1) [data,right of=df1,bpe] {$d\bx_{1}$};
	\node (dw1) [data,below of=df1,bpe] {$d\bw_1$};
	%
	\node (dx0) [data,left of=df1,bpe] {$d\bx_{0}$};
	%
	\draw [to,bp] (wL.south) -- (dfL.north) {};
	\draw [to,bp] (dfL.south) -- (dwL.north) {};
	\draw [to,bp] (dxL.west) -- (dfL.east) {};
	\draw [to,bp] (dfL.west) -- (dxLm.east) {};
	%
	\draw [to,bp] (dxLm.west) -- (df3.east) {};
	\draw [to,bp] (df3.west) -- (dx2.east) {};
	%
	\draw [to,bp] (w2.south) -- (df2.north) {};
	\draw [to,bp] (df2.south) -- (dw2.north) {};
	\draw [to,bp] (dx2.west) -- (df2.east) {};
	\draw [to,bp] (df2.west) -- (dx1.east) {};
	%
	\draw [to,bp] (w1.south) -- (df1.north) {};
	\draw [to,bp] (df1.south) -- (dw1.north) {};
	\draw [to,bp] (dx1.west) -- (df1.east) {};
	%
	\draw [to,bp] (df1.west) -- (dx0.east) {};
	%
	\draw [to,bp] (x0) -- (df1) {} ;
	\draw [to,bp] (x1) -- (df2) {} ;
	\draw [to,bp] (xLm) -- (dfL) {} ;
	\end{tikzpicture}
\end{center}

% ------------------------------------------------------------------
\subsubsection{Backpropagation in DAGs}\label{s:dag}
% ------------------------------------------------------------------

Assume that the DAG has a single output variable $\bx_L$ and assume, without loss of generality, that all variables are sorted in order of computation $(\bx_0,\bx_1,\dots,\bx_{L-1},\bx_L)$ according to the DAG structure. Furthermore, in order to simplify the notation, assume that this list contains both data and parameter variables, as the distinction is moot for the discussion in this section.

We can cut the DAG at any point in the sequence by fixing $\bx_0, \dots, \bx_{l-1}$ to some arbitrary value and dropping all the DAG layers that feed into them, effectively transforming the first $l$ variables into inputs. Then, the rest of the DAG defines a function $h_l$ that maps these input variables to the output $\bx_L$:
\[
\bx_L = h_l(\bx_0,\bx_1,\dots,\bx_{l-1}).
\]
Next, we show that backpropagation in a DAG iteratively computes the projected derivatives of all functions $h_1,\dots,h_L$ with respect to all their parameters.

Backpropagation starts by initializing variables $(d\bx_{0},\dots,d\bx_{l-1})$ to null tensors of the same size as $(\bx_0,\dots,\bx_{l-1})$. Next, it computes the projected derivatives of
\[
\bx_L = h_L(\bx_0,\bx_1,\dots,\bx_{L-1}) =
f_{\pi_L}(\bx_0,\bx_1,\dots,\bx_{L-1}).
\]
Here $\pi_l$ denotes the index of the layer $f_{\pi_l}$ that computes the value of the variable $\bx_l$. There is at most one such layer, or none if $\bx_l$ is an input or parameter of the original NN. In the first case, the layer may depend on any of the variables prior to $\bx_l$ in the sequence, so that general one has:
\[
\bx_{l} = f_{\pi_l}(\bx_0,\dots,\bx_{l-1}).
\]
At the beginning of backpropagation, since there are no intermediate variables between $\bx_{L-1}$ and $\bx_L$, the function $h_L$ is the same as the last layer $f_{\pi_L}$. Thus the projected derivatives of $h_L$ are the same as the projected derivatives of $f_{\pi_L}$, resulting in the equation
\[
\forall t=0,\dots,L-1:\qquad
d\bx_{t} \leftarrow d\bx_{t}
+ \frac{d\langle \bp_L, f_{\pi_L}(\bx_0,\dots,\bx_{t-1})\rangle}{d\bx_t}.
\]
Here, for uniformity with the other iterations, we use the fact that $d\bx_l$ are initialized to zero an\emph{accumulate} the values instead of storing them. In practice, the update operation needs to be carried out only for the variables $\bx_l$ that are actual inputs to $f_{\pi_L}$, which is often a tiny fraction of all the variables in the DAG.

After the update, each $d\bx_t$ contains the projected derivative of function $h_L$ with respect to the corresponding variable:
\[
\forall t=0,\dots,L-1:\qquad
d\bx_t = \frac{d\langle \bp_L, h_L(\bx_0,\dots,\bx_{l-1})\rangle}{d\bx_t}.
\]
Given this information, the next iteration of backpropagation updates the variables to contain the projected derivatives of $h_{L-1}$ instead. In general, given the derivatives of $h_{l+1}$, backpropagation computes the derivatives of $h_{l}$ by using the relation
\[
\bx_L
= 
h_{l}(\bx_0,\bx_1,\dots,\bx_{l-1})
=
h_{l+1}(\bx_0,\bx_1,\dots,\bx_{l-1},f_{\pi_L}(\bx_0,\dots,\bx_{l-1}))
\]
Applying the chain rule to this expression, for all $0\leq t \leq l-1$:
\[
\frac{d\langle \bp, h_l \rangle}{d(\vv \bx_t)^\top}
=
\frac{d\langle \bp, h_{l+1}\rangle}{d(\vv \bx_t)^\top}
+
\underbrace{\frac{d\langle \bp_L, h_{l+1}\rangle}{d(\vv \bx_l)^\top}}_{\vv d\bx_l}
\frac{d \vv f_{\pi_l}}{d(\vv \bx_t)^\top}.
\]
This yields the update equation
\begin{equation}\label{e:bp-update}	
\forall t=0,\dots,l-1:\qquad
d\bx_t \leftarrow d\bx_t + \frac{d\langle \bp_l, f_{\pi_l}(\bx_0,\dots,\bx_{l-1})\rangle}{d\bx_t},
\quad
\text{where\ }
\bp_l = d\bx_l.
\end{equation}
Once more, the update needs to be explicitly carried out only for the variables $\bx_t$ that are actual inputs of $f_{\pi_l}$. In particular, if $\bx_l$ is a data input or a parameter of the original neural network, then $\bx_l$ does not depend on any other variables or parameters and $f_{\pi_l}$ is a nullary function (i.e.\ a function with no arguments). In this case, the update does not do anything. 
After iteration $L-l+1$ completes, backpropagation remains with:
\begin{align*}
\forall t=0,\dots,l-1:&\qquad
d\bx_t
=
\frac{d\langle \bp_L, h_l(\bx_0,\dots,\bx_{l-1})\rangle}{d\bx_t}.
\end{align*}
Note that the derivatives for variables $\bx_t, l \leq t \leq L-1$ are not updated since $h_l$ does not depend on any of those. Thus, after all $L$ iterations are complete, backpropagation terminates with
\[
\forall l=1,\dots,L:\qquad
d\bx_{l-1}
=
\frac{d\langle \bp_L, h_{l}(\bx_0,\dots,\bx_{l-1})\rangle}{d\bx_{l-1}}.
\]
As seen above, functions $h_{l}$ are obtained from the original network $f$ by transforming variables $\bx_0,\dots,\bx_{l-1}$ into to inputs. If $\bx_{l-1}$ was already an input (data or parameter) of $f$, then the derivative $d\bx_{l-1}$ is applicable to $f$ as well.

Backpropagation can be summarized as follows:
\begin{center}
	\fbox{\begin{minipage}{0.95\textwidth}
			Given: a DAG neural network $f$ with a single output $\bx_L$, the values of all input variables (including the parameters), and the value of the projection $\bp_L$ (usually $\bx_L$ is a scalar and $\bp_L = p_L = 1$):
			\begin{enumerate}
				\item Sort all variables by computation order $(\bx_0,\bx_1,\dots,\bx_L)$ according to the DAG.
				\item Perform a forward pass through the network to compute all the intermediate variable values.
				\item Initialize $(d\bx_0, \dots, d\bx_{L-1})$ to null tensors with the same size as the corresponding variables.
				\item For $l=L,L-1,\dots,2,1$:
				\begin{enumerate}
					\item Find the index $\pi_l$ of the layer $\bx_{l} = f_{\pi_l}(\bx_0,\dots,\bx_{l-1})$ that evaluates variable $\bx_l$. If there is no such layer (because $\bx_{l}$ is an input or parameter of the network), go to the next iteration.
					\item Update the variables using the formula:
					\[
					\forall t=0,\dots,l-1:\qquad
					d\bx_t \leftarrow d\bx_t + \frac{d\langle d\bx_l, f_{\pi_l}(\bx_0,\dots,\bx_{l-1})\rangle}{d\bx_t}.
					\]
					To do so efficiently, use the ``backward mode'' of the layer $f_{\pi_l}$ to compute its derivative projected onto $d\bx_l$ as needed.
				\end{enumerate}
			\end{enumerate}
	\end{minipage}}
\end{center}

% TODO: what to do with multiple outputs


\begin{figure}[t]
	\begin{center}
		\begin{tikzpicture}[auto, node distance=0.3cm]
		\matrix (m) [matrix of math nodes, 
		column sep=1.2cm,
		row sep=0.3cm]
		{
			& \node (f1) [block]{f_1}; 
			& \node (x1) [datac]{\bx_1};
			\\
			\node (x0) [datac]{\bx_0};
			&
			&
			& \node (f3) [block]{f_3};
			& \node (x3) [datac]{\bx_3};
			\\
			& \node (f2) [block]{f_2}; 
			& \node (x2) [datac]{\bx_2};
			& &
			& \node (f5) [block]{f_5}; 
			& \node (x7) [datac]{\bx_7}; 
			\\
			& 
			& \node(x5) [datac]{\bx_5};
			\\
			\node (x4) [datac]{\bx_4};
			& \node (f4) [block]{f_4};
			\\
			& 
			& \node(x6) [datac]{\bx_6};
			\\
			% BP
			& \node (df1) [block,bp]{df_1}; 
			& \node (dx1) [datac,bp]{d\bx_1};
			\\
			\node (dx0) [datac,bp]{d\bx_0};
			&
			&
			& \node (df3) [block,bp]{df_3};
			& \node (dx3) [datac,bp]{d\bx_3};
			\\
			& \node (df2) [block,bp]{df_2}; 
			& \node (dx2) [datac,bp]{d\bx_2};
			& &
			& \node (df5) [block,bp]{df_5}; 
			& \node (dx7) [datac,bp]{\bp_7}; 
			\\
			& 
			& \node (dx5) [datac,bp]{d\bx_5};
			\\
			\node (dx4) [datac,bp]{d\bx_4};
			& \node (df4) [block,bp]{df_4};
			\\
			& 
			& \node(dx6) [datac,bp]{\bp_6};
			\\
		};
		\draw[to] (x0) -- (f1);
		\draw[to] (f1) -- (x1);
		\draw[to] (x1) -- (f3);
		\draw[to] (x0) -- (f2);
		\draw[to] (f2) -- (x2);
		\draw[to] (x2) -- (f3);
		\draw[to] (f3) -- (x3);
		\draw[to] (x3) -- (f5);
		\draw[to] (f5) -- (x7);
		\draw[to] (x4) -- (f4);
		\draw[to] (f4) -- (x5);
		\draw[to] (f4) -- (x6);
		\draw[to] (x5) -- (f5);
		\node(w1) [par,below=of f1]{$\bw_1$}; \draw[to] (w1) -- (f1);
		\node(w2) [par,below=of f2]{$\bw_2$}; \draw[to] (w2) -- (f2);
		\node(w4) [par,below=of f4]{$\bw_4$}; \draw[to] (w4) -- (f4);
		\draw[to] (w4) to [bend right] (f3);
		\node(w5) [par,below=of f5]{$\bw_5$}; \draw[to] (w5) -- (f5);
		\node (dx0s) [right of=dx0,xshift=20pt,draw,rectangle,bp]{$\Sigma$};
		\draw[from,bp] (dx0) -- (dx0s);
		\draw[from,bp] (dx0s) -- (df1);
		\draw[from,bp] (df1) -- (dx1);
		\draw[from,bp] (dx1) -- (df3);
		\draw[from,bp] (dx0s) -- (df2);
		\draw[from,bp] (df2) -- (dx2);
		\draw[from,bp] (dx2) -- (df3);
		\draw[from,bp] (df3) -- (dx3);
		\draw[from,bp] (dx3) -- (df5);
		\draw[from,bp] (df5) -- (dx7);
		\draw[from,bp] (dx4) -- (df4);
		\draw[from,bp] (df4) -- (dx5);
		\draw[from,bp] (df4) -- (dx6);
		\draw[from,bp] (dx5) -- (df5);
		\node(dw1) [par,below=of df1,bp]{$d\bw_1$}; \draw[from,bp] (dw1) -- (df1);
		\node(dw2) [par,below=of df2,bp]{$d\bw_2$}; \draw[from,bp] (dw2) -- (df2);
		\node(dw4s) [below of=df4,draw,rectangle,bp,yshift=-25pt]{$\Sigma$}; \draw[from,bp] (dw4s) -- (df4);
		\node(dw4) [par,below=of dw4s,bp]{$d\bw_4$}; \draw[from,bp] (dw4) -- (dw4s);
		\draw[from,bp] (dw4s) to [bend right,bp] (df3);
		\node(dw5) [par,below=of df5,bp]{$d\bw_5$}; \draw[from,bp] (dw5) -- (df5);
		%
		\draw[to,bpl] (x0) -| ([xshift=-0.3cm]x0.west) |- (df1);
		\draw[to,bpl] (x0) -| ([xshift=-0.6cm]x0.west) |- (df2);
		\draw[to,bpl] (x1) -| ([xshift=4cm]x1.west) |- ([yshift=10pt]df3.east);
		\draw[to,bpl] (x2) -| (df3);
		\draw[to,bpl] (x3) -| ([xshift=+5cm]x3.east) |- ([yshift=15pt]df5.east);
		\draw[to,bpl] (x4) to [bend right=75] ([yshift=15pt]df4.west);
		\draw[to,bpl] (x5) to [bend left] (df5);
		\end{tikzpicture}
	\end{center}
	\vspace{-1em}
	\caption{\textbf{Backpropagation network for a DAG.}}\label{f:dagbp}
\end{figure}

% ------------------------------------------------------------------
\subsubsection{DAG backpropagation networks}\label{s:bpnets-dag}
% ------------------------------------------------------------------

Just like for sequences, backpropagation in DAGs can be implemented as a corresponding BP-reversed DAG. To construct the reversed DAG:
\begin{enumerate}
	\item For each layer $f_l$, and variable/parameter $\bx_t$ and $\bw_l$, create a corresponding layer $df_l$ and variable/parameter $d\bx_t$ and $d\bw_l$.
	\item If a variable $\bx_t$ (or parameter $\bw_l$) is an input of $f_l$, then it is an input of $df_l$ as well.
	\item If a variable $\bx_t$ (or parameter $\bw_l$) is an input of $f_l$, then the variable $d\bx_t$ (or the parameter $d\bw_l$) is an output $df_l$.
	\item In the previous step, if a variable $\bx_t$ (or parameter $\bw_l$) is input to two or more layers in $f$, then $d\bx_t$ would be the output of two or more layers in the reversed network, which creates a conflict. Resolve these conflicts by inserting a summation layer that adds these contributions (this corresponds to the summation in the BP update equation \eqref{e:bp-update}).
\end{enumerate}
The BP network corresponding to the DAG of Fig.~\ref{f:dag} is given in Fig.~\ref{f:dagbp}.

% ------------------------------------------------------------------
\section{Learning tasks}\label{LearningTasks}
% ------------------------------------------------------------------

Learning tasks for neural networks can be classified according to the source of information for them. Fig.~\ref{LearningActivityDiagram}

There are basically two sources of information: data sets and mathematical models. 
In this way, some classes of learning tasks which learn from data sets are function regression, pattern recognition or time series prediction. 
On the other hand, learning tasks in which learning is performed from mathematical models are optimal control or optimal shape design. 
Finally, in inverse problems the neural network learns from both data sets and mathematical models.

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=0.35\textwidth]{neural_networks_basis/learning_problem}
		\caption{Learning problem for neural networks.}\label{LearningActivityDiagram}
	\end{center}
\end{figure}

\subsection{Function regression}

Function regression is the most popular learning task for neural networks. 
It is also called modelling. The function regression problem can be regarded as the problem of
approximating a function from a data set consisting of input-target instances
\cite{Haykin1994}. The targets are a specification
of what the response to the inputs should be \cite{Bishop1995}. 
While input variables might be quantitative or qualitative, in function regression target variables are quantitative. 

Performance measures for function regression are based on a sum of errors between the outputs from the neural network and the targets in the training data. 
As the training data is usually defficient, a regularization term might be required in order to solve the problem correctly.  

An example is to design an instrument that can determine serum cholesterol levels from
measurements of spectral content of a blood sample. There are a number of 
patients for which there are measurements of several wavelengths of the spectrum.
For the same patients there are also measurements of several
cholesterol levels, based on serum separation \cite{Demuth2009}. 

\subsection{Pattern recognition}

The learning task of pattern recognition gives raise to artificial intelligence. That problem can be stated as
the process whereby a received pattern, characterized by a distinct
set of features, is assigned to one of a prescribed number of
classes \cite{Haykin1994}. Pattern recognition is also known as classification. Here
the neural network learns from knowledge represented by a training
data set consisting of input-target instances. The inputs include a
set of features which characterize a pattern, and they can be quantitative or qualitative. The targets specify
the class that each pattern belongs to and therefore are qualitative \cite{Bishop1995}.

Classification problems can be, in fact, formulated as being modelling problems. 
As a consequence, performance functionals used here are also based on the sum squared error. 
Anyway, the learning task of pattern recognition is more difficult to solve than that of function regression. 
This means that a good knowledge of the state of the technique is recommended for success. 

A typical example is to disinguish hand-written versions of characters. 
Images of the characters might be captured and fed to a computer. 
An algorithm is then seek to which can distinguish as reliably as possible between the characters \cite{Bishop1995}. 

\subsection{Optimal control}

Optimal control is playing an increasingly important role in the
design of modern engineering systems. The aim
here is the optimization, in some defined sense, of a physical
process. More specifically, the objective of these problems is to
determine the control signals that will cause a process to satisfy
the physical constraints and at the same time minimize or maximize
some performance criterion \cite{Kirk1970} \cite{BalsaCanto2001}. 

The knowledge in optimal control problems is not represented in the form of a data set, it is given by a mathematical model. 
These objective functionals are often defined by integrals, ordinary differential equations or partial differential equations. 
In this way, and in order to evaluate them, we might need to apply Simpon methods, Runge-Kutta methods or finite element methods. 
Optimal control problems often include constraints. 

As a simple example, consider the problem of a rocket launching a
satellite into an orbit around the earth. An associated optimal
control problem is to choose the controls (the thrust attitude angle
and the rate of emission of the exhaust gases) so that the rocket
takes the satellite into its prescribed orbit with minimum
expenditure of fuel or in minimum time.

\subsection{Optimal shape design}

Optimal shape design is a very interesting field for industrial
applications. The goal in these problems
is to computerize the development process of some tool, and
therefore shorten the time it takes to create or to improve the
existing one. Being more precise, in an optimal shape design process
one wishes to optimize some performance criterium involving the
solution of a mathematical model with respect to its domain of
definition \cite{Bucur2005}. 

As in the previous case, the neural network here learns from a mathematical model. 
Evaluation of the performance functional here might also need the integration of functions, ordinary differential equations or partial differential equations. 
Optimal shape design problems defined by partial differential equations are challenging applications. 

One example is the design of airfoils,
which proceeds from a knowledge of computational fluid dynamics
\cite{Eyi1994} \cite{Mohammadi2004}. The performance goal here might
vary, but increasing lift and reducing drag are among the most common. Other objectives as weight reduction, stress reinforcement and
even noise reduction can be obtained. On the other hand, the airfoil
may be required to achieve this performance with constraints on
thickness, pitching moment, etc.

\subsection{Inverse problems}

Inverse problems can be described as being opposed to direct
problems. In a direct problem the cause is given, and the effect is
determined. In an inverse problem the effect is given, and the cause
is estimated \cite{Kirsch1996} \cite{Sabatier2000} \cite{Ramm2005}.
There are two main types of inverse problems: input estimation, in
which the system properties and output are known and the input is to
be estimated; and properties estimation, in which the the system
input and output are known and the properties are to be estimated.
Inverse problems can be found in many areas of science and
engineering. 

This type of problems is of great interest from both a theoretical and practical perspectives. 
From a theoretical point of view, the neural network here needs both mathematical models and data sets. 
The aim is usually formulated as to find properties or inputs which make a mathematical model to comply with the data set. 
From a practical point of view, most numerical software must be tuned up before being on production. 
That means that the particular properties of a system must be properly estimated in order to simulate it well.

A typical inverse problem in geophysics is to find the 
subsurface inhomogeneities from collected scattered fields caused by
acoustic waves sent at the surface and a mathematical model of soil mechanics.  

\subsection{Tasks companion diagram}

As we have said, the knowledge for a neural network can be represented in the form of data sets or mathematical models. 
The neural network learns from data sets in function regression and pattern recognition; 
it learns from mathematical models in optimal control and optimal shape design; 
and it learns from both mathematical models and data sets in inverse problems. 
Please note that other possible applications can be added to these learning tasks. 

Figure \ref{LearningTasksFigure} shows the learning tasks for neural networks described in this section. 
As we can see, they are capable of dealing with a great range of applications. 
Any of that learning tasks is formulated as being a variational problem. 
All of them are solved using the three step approach described in the previous section. 
Modelling and classification are the most traditional; 
optimal control, optimal shape design and inverse problems can also be very useful. 

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=1.0\textwidth]{neural_networks_basis/learning_tasks}
		\caption{Learning tasks for neural networks.}\label{LearningTasksFigure}
	\end{center}
\end{figure}


% ------------------------------------------------------------------
\section{Computational blocks}\label{s:blocks}
% ------------------------------------------------------------------

This chapters describes the individual computational blocks supported by \matconvnet. The interface of a CNN computational block $<block>$ is designed after the discussion in \cref{s:fundamentals}. The block is implemented as a MATLAB function $y = vl_nn<block>(x,w)$ that takes as input MATLAB arrays $x$ and $w$ representing the input data and parameters and returns an array $y$ as output. In general, $x$ and $y$ are 4D real arrays packing $N$ maps or images, as discussed above, whereas $w$ may have an arbitrary shape.

The function implementing each block is capable of working in the backward direction as well, in order to compute derivatives. This is done by passing a third optional argument $dzdy$ representing the derivative of the output of the network with respect to $\by$; in this case, the function returns the derivatives $[dzdx,dzdw] = vl_nn<block>(x,w,dzdy)$ with respect to the input data and parameters. The arrays $dzdx$, $dzdy$ and $dzdw$ have the same dimensions of $x$, $y$ and $w$ respectively (see \cref{s:back}).

Different functions may use a slightly different syntax, as needed: many functions can take additional optional arguments, specified as property-value pairs; some do not have parameters  $w$ (e.g. a rectified linear unit); others can take multiple inputs and parameters, in which case there may be more than one $x$, $w$, $dzdx$, $dzdy$ or $dzdw$. See the rest of the chapter and MATLAB inline help for details on the syntax.\footnote{Other parts of the library will wrap these functions into objects with a perfectly uniform interface; however, the low-level functions aim at providing a straightforward and obvious interface even if this means differing slightly from block to block.}

The rest of the chapter describes the blocks implemented in \matconvnet, with a particular focus on their analytical definition. Refer instead to MATLAB inline help for further details on the syntax.

% ------------------------------------------------------------------
\subsection{Convolution}\label{s:convolution}
% ------------------------------------------------------------------

\begin{figure}[t]
	\centering
	\includegraphics[width=0.7\textwidth]{figures/svg/conv}
	\caption{\textbf{Convolution.} The figure illustrates the process of filtering a 1D signal $\bx$ by a filter $f$ to obtain a signal $\by$. The filter has $H'=4$ elements and is applied with a stride of $S_h =2$ samples. The purple areas represented padding $P_-=2$ and $P_+=3$ which is zero-filled. Filters are applied in a sliding-window manner across the input signal. The samples of $\bx$ involved in the calculation of a sample of $\by$ are shown with arrow. Note that the rightmost sample of $\bx$  is never processed by any filter application due to the sampling step. While in this case the sample is in the padded region, this can happen also without padding.}\label{f:conv}
\end{figure}

The convolutional block is implemented by the function $vl_nnconv$. $y=vl_nnconv(x,f,b)$ computes the convolution of the input map $\bx$ with a bank of $D''$ multi-dimensional filters $\bff$ and biases $b$. Here
\[
\bx\in\real^{H \times W \times D}, \quad
\bff\in\real^{H' \times W' \times D \times D''}, \quad
\by\in\real^{H'' \times W'' \times D''}.
\]
The process of convolving a signal is illustrated in \cref{f:conv} for a 1D slice. Formally, the output is given by
\[
y_{i''j''d''}
=
b_{d''}
+
\sum_{i'=1}^{H'}
\sum_{j'=1}^{W'}
\sum_{d'=1}^D
f_{i'j'd} \times x_{i''+i'-1,j''+j'-1,d',d''}.
\]
The call $vl_nnconv(x,f,[])$ does not use the biases. Note that the function works with arbitrarily sized inputs and filters (as opposed to, for example, square images). See \cref{s:impl-convolution} for technical details.

\paragraph{Padding and stride.} $vl_nnconv$ allows to specify  top-bottom-left-right paddings $(P_h^-,P_h^+,P_w^-,P_w^+)$ of the input array and subsampling strides $(S_h,S_w)$ of the output array:
\[
y_{i''j''d''}
=
b_{d''}
+
\sum_{i'=1}^{H'}
\sum_{j'=1}^{W'}
\sum_{d'=1}^D
f_{i'j'd} \times x_{S_h (i''-1)+i'-P_h^-, S_w(j''-1)+j' - P_w^-,d',d''}.
\]
In this expression, the array $\bx$ is implicitly extended with zeros as needed.

\paragraph{Output size.} $vl_nnconv$ computes only the ``valid'' part of the convolution; i.e. it requires each application of a filter to be fully contained in the input support.  The size of the output is computed in \cref{s:receptive-simple-filters} and is given by:
\[
H'' = 1 + \left\lfloor \frac{H - H' + P_h^- + P_h^+}{S_h} \right\rfloor.
\]
Note that the padded input must be at least as large as the filters: $H +P_h^- + P_h^+ \geq H'$, otherwise an error is thrown.

\paragraph{Receptive field size and geometric transformations.} Very often it is useful to geometrically relate the indexes of the various array to the input data (usually images) in terms of coordinate transformations and size of the receptive field (i.e. of the image region that affects an output). This is derived in \cref{s:receptive-simple-filters}.

\paragraph{Fully connected layers.} In other libraries, \emph{fully connected blocks or layers} are linear functions where each output dimension depends on all the input dimensions. \matconvnet does not distinguish between fully connected layers and convolutional blocks. Instead, the former is a special case of the latter obtained when the output map $\by$ has dimensions $W''=H''=1$. Internally, $vl_nnconv$ handles this case more efficiently when possible.

\paragraph{Filter groups.} For additional flexibility, $vl_nnconv$ allows to group channels of the input array $\bx$ and apply different subsets of filters to each group. To use this feature, specify as input a bank  of $D''$ filters $\bff\in\real^{H'\times W'\times D'\times D''}$ such that $D'$ divides the number of input dimensions $D$. These are treated as $g=D/D'$ filter groups; the first group is applied to dimensions $d=1,\dots,D'$ of the input $\bx$; the second group to dimensions $d=D'+1,\dots,2D'$ and so on. Note that the output is still an array $\by\in\real^{H''\times W''\times D''}$.

An application of grouping is implementing the Krizhevsky and Hinton network~\cite{krizhevsky12imagenet} which uses two such streams. Another application is sum pooling; in the latter case, one can specify $D$ groups of $D'=1$ dimensional filters identical filters of value 1 (however, this is considerably slower than calling the dedicated pooling function as given in \cref{s:pooling}).

\paragraph{Dilation.} $vl_nnconv$ allows kernels to be spatially dilated on the fly by inserting zeros between elements. For instance, a dilation factor $d=2$ causes the 1D kernel $[f_1,f_2]$ to be implicitly transformed into the kernel $[f_1,0,f_2]$. Under this notation, $d-1$ zeros are inserted between filter elements (and consequently, a dilation factor of $1$ has no effect). Thus, with dilation factors $d_h,d_w$, a filter of size $(H_f,W_f)$ is equivalent to a filter of size:
\[
H' = d_h(H_f - 1) + 1,
\qquad
W' = d_w(W_f - 1) + 1.
\]
With dilation, the convolution becomes:
\[
y_{i''j''d''}
=
b_{d''}
+
\sum_{i'=1}^{H_f}
\sum_{j'=1}^{W_f}
\sum_{d'=1}^D
f_{i'j'd} \times x_{
	S_h (i''-1)+d_h(i'-1)-P_h^-+1,
	S_w (j''-1)+d_w(j'-1)-P_w^-+1,
	d',d''}.
\]


% ------------------------------------------------------------------
\subsection{Convolution transpose (deconvolution)}\label{s:convt}
% ------------------------------------------------------------------

\begin{figure}[t]
	\centering
	\includegraphics[width=0.7\textwidth]{figures/svg/convt}
	\caption{\textbf{Convolution transpose.} The figure illustrates the process of filtering a 1D signal $x$ by a filter $f$ to obtain a signal $y$. The filter is applied as a sliding-window, forming a pattern which is the transpose of the one of \cref{f:conv}. The filter has $H'=4$ samples in total, although each filter application uses two of them (blue squares) in a circulant manner. The purple areas represent crops with $C_-=2$ and $C_+=3$ which are discarded. The arrows exemplify which samples of $x$ are involved in the calculation of a particular sample of $y$. Note that, differently from the forward convolution \cref{f:conv}, there is no need to add padding to the input array; instead, the convolution transpose filters can be seen as being applied with maximum input padding (more would result in zero output values), and the latter can be reduced by cropping the output instead.}\label{f:convt}
\end{figure}

The \emph{convolution transpose} block (sometimes referred to as ``deconvolution'') is the transpose of the convolution block described in \cref{s:convolution}. In \matconvnet, convolution transpose is  implemented by the function $vl_nnconvt$.

In order to understand convolution transpose, let:
\[
\bx\in\real^{H \times W \times D}, \quad
\bff\in\real^{H' \times W' \times D \times D''}, \quad
\by\in\real^{H'' \times W'' \times D''}, \quad
\]
be the input tensor, filters, and output tensors. Imagine operating in the reverse direction by using the filter bank $\bff$ to convolve the output $\by$ to obtain the input $\bx$, using the definitions given in~\cref{s:convolution} for the convolution operator; since convolution is linear, it can be expressed as a matrix $M$ such that  $\vv \bx = M \vv\by$; convolution transpose computes instead $\vv \by = M^\top \vv \bx$. This process is illustrated for a 1D slice in \cref{f:convt}.

There are two important applications of convolution transpose. The first one is the so called \emph{deconvolutional networks}~\cite{zeiler14visualizing} and other networks such as convolutional decoders that use the transpose of a convolution. The second one is implementing data interpolation. In fact, as the convolution block supports input padding and output downsampling, the convolution transpose block supports input upsampling and output cropping.

Convolution transpose can be expressed in closed form in the following rather unwieldy expression (derived in \cref{s:impl-convolution-transpose}):
\begin{multline}\label{e:convt}
y_{i''j''d''} =
\sum_{d'=1}^{D}
\sum_{i'=0}^{q(H',S_h)}
\sum_{j'=0}^{q(W',S_w)}
f_{
	1+ S_hi' + m(i''+ P_h^-, S_h),\ %
	1+ S_wj' + m(j''+ P_w^-, S_w),\ %
	d'',
	d'
}
\times \\
x_{
	1 - i' + q(i''+P_h^-,S_h),\ %
	1 - j' + q(j''+P_w^-,S_w),\ %
	d'
}
\end{multline}
where
\[
m(k,S) = (k - 1) \bmod S,
\qquad
q(k,n) = \left\lfloor \frac{k-1}{S} \right\rfloor,
\]
$(S_h,S_w)$ are the vertical and horizontal \emph{input upsampling factors},  $(P_h^-,P_h^+,P_h^-,P_h^+)$ the \emph{output crops}, and $\bx$ and $\bff$ are zero-padded as needed in the calculation. Note also that filter $k$ is stored as a slice $\bff_{:,:,k,:}$ of the 4D tensor $\bff$.

The height of the output array $\by$ is given by
\[
H'' = S_h (H - 1) + H' -P^-_h - P^+_h.
\]
A similar formula holds true for the width. These formulas are derived in \cref{s:receptive-convolution-transpose} along with an expression for the receptive field of the operator.

We now illustrate the action of convolution transpose in an example (see also \cref{f:convt}).  Consider a 1D slice in the vertical direction, assume that the crop parameters are zero, and that $S_h>1$. Consider the output sample $y_{i''}$ where the index $i''$ is chosen such that $S_h$ divides $i''-1$; according to~\eqref{e:convt}, this sample is obtained as a weighted summation of $x_{i'' / S_h},x_{i''/S_h-1},...$ (note that the order is reversed). The weights are the filter elements $f_1$, $f_{S_h}$,$f_{2S_h},\dots$ subsampled with a step of $S_h$. Now consider computing the element $y_{i''+1}$; due to the rounding in the quotient operation $q(i'',S_h)$, this output sample is obtained as a weighted combination of the same elements of the input $x$ that were used to compute $y_{i''}$; however, the filter weights are now shifted by one place to the right: $f_2$, $f_{S_h+1}$,$f_{2S_h+1}$, $\dots$. The same is true for $i''+2, i'' + 3,\dots$ until we hit $i'' + S_h$. Here the cycle restarts after shifting $\bx$ to the right by one place. Effectively, convolution transpose works as an \emph{interpolating filter}.

% ------------------------------------------------------------------
\subsection{Spatial pooling}\label{s:pooling}
% ------------------------------------------------------------------

$vl_nnpool$ implements max and sum pooling. The \emph{max pooling} operator computes the maximum response of each feature channel in a $H' \times W'$ patch
\[
y_{i''j''d} = \max_{1\leq i' \leq H', 1 \leq j' \leq W'} x_{i''+i'-1,j''+j'-1,d}.
\]
resulting in an output of size $\by\in\real^{H''\times W'' \times D}$, similar to the convolution operator of \cref{s:convolution}. Sum-pooling computes the average of the values instead:
\[
y_{i''j''d} = \frac{1}{W'H'}
\sum_{1\leq i' \leq H', 1 \leq j' \leq W'} x_{i''+i'-1,j''+j'-1,d}.
\]
Detailed calculation of the derivatives is provided in \cref{s:impl-pooling}.

\paragraph{Padding and stride.} Similar to the convolution operator of \cref{s:convolution}, $vl_nnpool$ supports padding the input; however, the effect is different from padding in the convolutional block as pooling regions straddling the image boundaries are cropped. For max pooling, this is equivalent to extending the input data with $-\infty$; for sum pooling, this is similar to padding with zeros, but the normalization factor at the boundaries is smaller to account for the smaller integration area.

% ------------------------------------------------------------------
\subsection{Activation functions}\label{s:activation}
% ------------------------------------------------------------------

\matconvnet supports the following activation functions:
%
\begin{itemize}
	\item \emph{ReLU.} $vl_nnrelu$ computes the \emph{Rectified Linear Unit} (ReLU):
	\[
	y_{ijd} = \max\{0, x_{ijd}\}.
	\]
	
	\item \emph{Sigmoid.} $vl_nnsigmoid$ computes the \emph{sigmoid}:
	\[
	y_{ijd} = \sigma(x_{ijd}) = \frac{1}{1+e^{-x_{ijd}}}.
	\]
\end{itemize}
%
See \cref{s:impl-activation} for implementation details.

% ------------------------------------------------------------------
\subsection{Spatial bilinear resampling}\label{s:spatial-sampler}
% ------------------------------------------------------------------

$vl_nnbilinearsampler$ uses bilinear interpolation to spatially warp the image according to an input transformation grid. This operator works with an input image $\bx$, a grid $\bg$, and an output image $\by$ as follows:
\[
\bx \in \mathbb{R}^{H \times W \times C},
\qquad
\bg \in [-1,1]^{2 \times H' \times W'},
\qquad
\by \in \mathbb{R}^{H' \times W' \times C}.
\]
The same transformation is applied to all the features channels in the input, as follows:
\begin{equation}\label{e:bilinear}
y_{i''j''c}
=
\sum_{i=1}^H
\sum_{j=1}^W
x_{ijc}
\max\{0, 1-|\alpha_v g_{1i''j''} + \beta_v - i|\}
\max\{0, 1-|\alpha_u g_{2i''j''} + \beta_u - j|\},
\end{equation}
where, for each feature channel $c$, the output $y_{i''j''c}$ at the location $(i'',j'')$, is a weighted sum of the input values $x_{ijc}$ in the neighborhood of location $(g_{1i''j''},g_{2i''j''})$. The weights, as given in \eqref{e:bilinear}, correspond to performing bilinear interpolation. Furthermore, the grid coordinates are expressed not in pixels, but relative to a reference frame that extends from $-1$ to $1$ for all spatial dimensions of the input image; this is given by choosing the coefficients as:
\[
\alpha_v = \frac{H-1}{2},\quad
\beta_v = -\frac{H+1}{2},\quad
\alpha_u = \frac{W-1}{2},\quad
\beta_u = -\frac{W+1}{2}.
\]

See \cref{s:impl-sampler} for implementation details.

% ------------------------------------------------------------------
\subsection{Region of interest pooling}\label{s:roi-pooling}
% ------------------------------------------------------------------

The \emph{region of interest (ROI) pooling} block applies max or average pooling to specified subwindows of a tensor. A region is a rectangular region $R = (u_-,v_-,u_+,v_+)$. The region itself is partitioned into $(H',W')$ tiles along the vertical and horizontal directions. The edges of the tiles have coordinates
\begin{align*}
v_{i'} &= v_- + (v_+ - v_- + 1) (i' - 1), \quad i' = 1,\dots,H',\\
u_{j'} &= u_- + (u_+ - u_- + 1) (j' - 1), \quad j' = 1,\dots,W'.
\end{align*}
Following the implementation of~\cite{girshick15fast}, the $H'\times W'$ pooling tiles are given by
\[
\Omega_{i'j'} =
\{\lfloor v_{i'} \rfloor + 1, \dots, \lceil v_{i'+1} \rceil\}
\times
\{\lfloor u_{i'} \rfloor + 1, \dots, \lceil u_{i'+1} \rceil\}.
\]
Then the input and output tensors are as follows:
\[
\bx \in \mathbb{R}^{H \times W \times C},
\qquad
\by \in \mathbb{R}^{H' \times W' \times C},
\]
where
\[
y_{i'j'c} = \operatornamewithlimits{max}_{(i,j) \in \Omega_{i'j'}} x_{ijc}.
\]
Alternatively, $\max$ can be replaced by the averaging operator.

The extent of each region is defined by four coordinates as specified above; however, differently from tensor indexes, these use $(0,0)$ as the coordinate of the top-left pixel. In fact, if there is a single tile ($H'=W'=1$), then the region $(0,0,H-1,W-1)$ covers the whole input image:
\[
\Omega_{11} =
\{1, \dots, W\}
\times
\{1, \dots, H\}.
\]

In more details, the input of the block is a sequence of $K$ regions. Each region pools one of the $T$ images in the batch stored in $\bx \in \mathbb{R}^{H\times W\times C\times T}$. Regions are therefore specified as a tensor $R \in \mathbb{R}^{5 \times K}$, where the first coordinate is the index of the pooled image in the batch. The output is a $\by \in \mathbb{R}^{H' \times W' \times C \times K}$ tensor.

For compatibility with~\cite{girshick15fast}, furthermore, the region coordinates are rounded to the nearest integer before the definitions above are used. Note also that, due to the discretization details, 1) tiles always contain at least one pixel, 2) there can be a pixel of overlap between them and 3) the discretization has a slight bias towards left-top pixels.

% ------------------------------------------------------------------
\subsection{Normalization}\label{s:normalization}
% ------------------------------------------------------------------

% ------------------------------------------------------------------
\subsubsection{Local response normalization (LRN)}\label{s:ccnormalization}
% ------------------------------------------------------------------

$vl_nnnormalize$ implements the Local Response Normalization (LRN) operator. This operator is applied independently at each spatial location and to groups of feature channels as follows:
\[
y_{ijk} = x_{ijk} \left( \kappa + \alpha \sum_{t\in G(k)} x_{ijt}^2 \right)^{-\beta},
\]
where, for each output channel $k$, $G(k) \subset \{1, 2, \dots, D\}$ is a corresponding subset of input channels. Note that input $\bx$ and output $\by$ have the same dimensions. Note also that the operator is applied uniformly at all spatial locations.

See \cref{s:impl-ccnormalization} for implementation details.

% ------------------------------------------------------------------
\subsubsection{Batch normalization}\label{s:bnorm}
% ------------------------------------------------------------------

$vl_nnbnorm$ implements batch normalization~\cite{ioffe2015}. Batch normalization is somewhat different from other neural network blocks in that it performs computation across images/feature maps in a batch (whereas most blocks process different images/feature maps individually). $y = vl_nnbnorm(x, w, b)$ normalizes each channel of the feature map $\mathbf{x}$ averaging over spatial locations and batch instances. Let $T$ be the batch size; then
\[
\mathbf{x}, \mathbf{y} \in \mathbb{R}^{H \times W \times K \times T},
\qquad\mathbf{w} \in \mathbb{R}^{K},
\qquad\mathbf{b} \in \mathbb{R}^{K}.
\]
Note that in this case the input and output arrays are explicitly treated as 4D tensors in order to work with a batch of feature maps. The tensors  $\mathbf{w}$ and $\mathbf{b}$ define component-wise multiplicative and additive constants. The output feature map is given by
\[
y_{ijkt} = w_k \frac{x_{ijkt} - \mu_{k}}{\sqrt{\sigma_k^2 + \epsilon}} + b_k,
\quad
\mu_{k} = \frac{1}{HWT}\sum_{i=1}^H \sum_{j=1}^W \sum_{t=1}^{T} x_{ijkt},
\quad
\sigma^2_{k} = \frac{1}{HWT}\sum_{i=1}^H \sum_{j=1}^W \sum_{t=1}^{T} (x_{ijkt} - \mu_{k})^2.
\]
See \cref{s:impl-bnorm} for implementation details.

% ------------------------------------------------------------------
\subsubsection{Spatial normalization}\label{s:spnorm}
% ------------------------------------------------------------------

$vl_nnspnorm$ implements spatial normalization. The spatial normalization operator acts on different feature channels independently and rescales each input feature by the energy of the features in a local neighbourhood . First, the energy of the features in a neighbourhood $W'\times H'$ is evaluated
\[
n_{i''j''d}^2 = \frac{1}{W'H'}
\sum_{1\leq i' \leq H', 1 \leq j' \leq W'} x^2_{
	i''+i'-1-\lfloor \frac{H'-1}{2}\rfloor,
	j''+j'-1-\lfloor \frac{W'-1}{2}\rfloor,
	d}.
\]
In practice, the factor $1/W'H'$ is adjusted at the boundaries to account for the fact that neighbors must be cropped. Then this is used to normalize the input:
\[
y_{i''j''d} = \frac{1}{(1 + \alpha n_{i''j''d}^2)^\beta} x_{i''j''d}.
\]
See \cref{s:impl-spnorm} for implementation details.

% ------------------------------------------------------------------
\subsubsection{Softmax}\label{s:softmax}
% ------------------------------------------------------------------

$vl_nnsoftmax$ computes the softmax operator:
\[
y_{ijk} = \frac{e^{x_{ijk}}}{\sum_{t=1}^D e^{x_{ijt}}}.
\]
Note that the operator is applied across feature channels and in a convolutional manner at all spatial locations. Softmax can be seen as the combination of an activation function (exponential) and a normalization operator. See \cref{s:impl-softmax} for implementation details.

% ------------------------------------------------------------------
\subsection{Categorical losses}\label{s:losses}
% ------------------------------------------------------------------

The purpose of a categorical loss function $\ell(\bx,\bc)$ is to compare a prediction $\bx$ to a ground truth class label $\bc$. As in the rest of \matconvnet, the loss is treated as a convolutional operator, in the sense that the loss is evaluated independently at each spatial location. However, the contribution of different samples are summed together (possibly after weighting) and the output of the loss is a scalar. \Cref{s:loss-classification} losses useful for multi-class classification and the \cref{s:loss-attributes} losses useful for binary attribute prediction. Further technical details are in \cref{s:impl-losses}. $vl_nnloss$ implements the following all of these.

% ------------------------------------------------------------------
\subsubsection{Classification losses}\label{s:loss-classification}
% ------------------------------------------------------------------

Classification losses decompose additively as follows:
\begin{equation}\label{e:addloss}
\ell(\bx,\bc) = \sum_{ijn} w_{ij1n} \ell(\bx_{ij:n}, \bc_{ij:n}).
\end{equation}
Here $\bx \in \mathbb{R}^{H \times W \times C \times N}$ and $\bc \in \{1, \dots, C\}^{H \times W \times 1 \times N}$, such that the slice $\bx_{ij:n}$ represent a vector of $C$ class scores and and $c_{ij1n}$ is the ground truth class label. The $`instanceWeights`$ option can be used to specify the tensor $\bw$ of weights, which are otherwise set to all ones; $\bw$ has the same dimension as $\bc$.

Unless otherwise noted, we drop the other indices and denote by $\bx$ and $c$  the slice $\bx_{ij:n}$ and the scalar $c_{ij1n}$. $vl_nnloss$ automatically skips all samples such that $c=0$, which can be used as an ``ignore'' label.

\paragraph{Classification error.} The classification error is zero if class $c$ is assigned the largest score and zero otherwise:
\begin{equation}\label{e:loss-classerror}
\ell(\bx,c) = \mathbf{1}\left[c \not= \argmax_k x_c\right].
\end{equation}
Ties are broken randomly.

\paragraph{Top-$K$ classification error.} The top-$K$ classification error is zero if class $c$ is within the top $K$ ranked scores:
\begin{equation}\label{e:loss-classerror}
\ell(\bx,c) = \mathbf{1}\left[ |\{k : x_k \geq x_c \}| \leq K \right].
\end{equation}
The classification error is the same as the top-$1$ classification error.

\paragraph{Log loss or negative posterior log-probability.} In this case, $\bx$ is interpreted as a vector of posterior probabilities $p(k) = x_k, k=1,\dots, C$ over the $C$ classes. The loss is the negative log-probability of the ground truth class:
\begin{equation}\label{e:loss-log}
\ell(\bx, c) = - \log x_c.
\end{equation}
Note that this makes the implicit assumption $\bx \geq 0, \sum_k x_k = 1$. Note also that, unless $x_c > 0$, the loss is undefined. For these reasons, $\bx$ is usually the output of a block such as softmax that can guarantee these conditions. However, the composition of the naive log loss and softmax is numerically unstable. Thus this is implemented as a special case below.

Generally, for such a loss to make sense, the score $x_c$ should be somehow in competition with the other scores $x_k, k\not = c$. If this is not the case, minimizing \eqref{e:loss-log} can trivially be achieved by maxing all $x_k$ large, whereas the intended effect is that $x_c$ should be large compared to the $x_k, k\not=c$. The softmax block makes the score compete through the normalization factor.

\paragraph{Softmax log-loss or multinomial logistic loss.} This loss combines the softmax block and the log-loss block into a single block:
\begin{equation}\label{e:loss-softmaxlog}
\ell(\bx, c) = - \log \frac{e^{x_c}}{\sum_{k=1}^C e^{x_k}}
= - x_c + \log \sum_{k=1}^C e^{x_k}.
\end{equation}
Combining the two blocks explicitly is required for numerical stability. Note that, by combining the log-loss with softmax, this loss automatically makes the score compete: $\ell(bx,c) \approx 0$ when $x_c \gg \sum_{k\not= c} x_k$.

This loss is implemented also in the \emph{deprecated} function $vl_softmaxloss$.

\paragraph{Multi-class hinge loss.} The multi-class logistic loss is given by
\begin{equation}\label{e:loss-multiclasshinge}
\ell(\bx, c) = \max\{0, 1 - x_c \}.
\end{equation}
Note that $\ell(\bx,c) =0 \Leftrightarrow x_c \geq 1$. This, just as for the log-loss above, this loss does not automatically make the score competes. In order to do that, the loss is usually preceded by the block:
\[
y_c = x_c - \max_{k \not= c} x_k.
\]
Hence $y_c$ represent the \emph{confidence margin} between class $c$ and the other classes $k \not= c$. Just like softmax log-loss combines softmax and loss, the next loss combines margin computation and hinge loss.

\paragraph{Structured multi-class hinge loss.} The structured multi-class logistic loss, also know as Crammer-Singer loss, combines the multi-class hinge loss with a block computing the score margin:
\begin{equation}\label{e:loss-structuredmulticlasshinge}
\ell(\bx, c) = \max\left\{0, 1 - x_c + \max_{k \not= c} x_k\right\}.
\end{equation}

% ------------------------------------------------------------------
\subsubsection{Attribute losses}\label{s:loss-attributes}
% ------------------------------------------------------------------

Attribute losses are similar to classification losses, but in this case classes are not mutually exclusive; they are, instead, binary attributes. Attribute losses decompose additively as follows:
\begin{equation}\label{e:addlossattribute}
\ell(\bx,\bc) = \sum_{ijkn} w_{ijkn} \ell(\bx_{ijkn}, \bc_{ijkn}).
\end{equation}
Here $\bx\in \mathbb{R}^{H \times W \times C \times N}$ and $\bc \in \{-1,+1\}^{H \times W \times C \times N}$, such that the scalar $x_{ijkn}$ represent a confidence that attribute $k$ is on and $c_{ij1n}$ is the ground truth attribute label. The $`instanceWeights`$ option can be used to specify the tensor $\bw$ of weights, which are otherwise set to all ones; $\bw$ has the same dimension as $\bc$.

Unless otherwise noted, we drop the other indices and denote by $x$ and $c$  the scalars $x_{ijkn}$ and  $c_{ijkn}$. As before, samples with $c=0$ are skipped.

\paragraph{Binary error.} This loss is zero only if the sign of $x - \tau$ agrees with the ground truth label $c$:
\begin{equation}\label{e:loss-binary}
\ell(x,c|\tau) = \mathbf{1}[\sign(x-\tau) \not= c].
\end{equation}
Here $\tau$ is a configurable threshold, often set to zero.

\paragraph{Binary log-loss.} This is the same as the multi-class log-loss but for binary attributes. Namely, this time $x_k \in [0,1]$ is interpreted as the probability that attribute $k$ is on:
\begin{align}\label{e:loss-binarylogloss}
\ell(x,c)
&=
\begin{cases}
- \log x, & c = +1, \\
- \log (1 - x), & c = -1, \\
\end{cases}
\\
&=
- \log \left[ c \left(x - \frac{1}{2}\right) + \frac{1}{2} \right].
\end{align}
Similarly to the multi-class log loss, the assumption $x \in [0,1]$ must be enforced by the block computing $x$.

\paragraph{Binary logistic loss.} This is the same as the multi-class logistic loss, but this time $x/2$ represents the confidence that the attribute is on and $-x/2$ that it is off. This is obtained by using the logistic function $\sigma(x)$
\begin{equation}\label{e:loss-binarylogistic}
\ell(x,c)
=
- \log \sigma(cx)
=
-\log \frac{1}{1 + e^{-{cx}}}
=
-\log \frac{e^{\frac{cx}{2}}}{e^{\frac{cx}{2}} + e^{-\frac{cx}{2}}}.
\end{equation}

\paragraph{Binary hinge loss.} This is the same as the structured multi-class hinge loss but for binary attributes:
\begin{equation}\label{e:loss-hinge}
\ell(x,c)
=
\max\{0, 1 - cx\}.
\end{equation}
There is a relationship between the hinge loss and the structured multi-class hinge loss which is analogous to the relationship between binary logistic loss and multi-class logistic loss. Namely, the hinge loss can be rewritten as:
\[
\ell(x,c) = \max\left\{0, 1 - \frac{cx}{2} + \max_{k\not= c} \frac{kx}{2}\right\}
\]
Hence the hinge loss is the same as the structure multi-class hinge loss for $C=2$ classes, where $x/2$ is the score associated to class $c=1$ and $-x/2$ the score associated to class $c=-1$.

% ------------------------------------------------------------------
\subsection{Comparisons}\label{s:comparisons}
% ------------------------------------------------------------------

% ------------------------------------------------------------------
\subsubsection{$p$-distance}\label{s:pdistance}
% ------------------------------------------------------------------

The $vl_nnpdist$ function computes the $p$-distance between the vectors in the input data $\bx$ and a target $\bar\bx$:
\[
y_{ij} = \left(\sum_d |x_{ijd} - \bar x_{ijd}|^p\right)^\frac{1}{p}
\]
Note that this operator is applied convolutionally, i.e. at each spatial location $ij$ one extracts and compares vectors $x_{ij:}$. By specifying the option $'noRoot', true$ it is possible to compute a variant omitting the root:
\[
y_{ij} = \sum_d |x_{ijd} - \bar x_{ijd}|^p, \qquad p > 0.
\]
See \cref{s:impl-pdistance} for implementation details.

%% ------------------------------------------------------------------
%\subsubsection{Product}\label{s:product}
%% ------------------------------------------------------------------
%
%\[
% y_{ijd} = x^{(1)}_{ijd} x^{(2)}_{ijd}
%\]
%
%\paragraph{Implementation details.}
%\[
% \frac{dz}{dx^{(1)}_{ijd}}
%  =
% \sum_{i''j''d''}
%  \frac{dz}{dy_{i''j''d''}}
%  \frac{dy_{i''j''d''}}{dx^{(1)}_{ijd}}
%  =
%  \frac{dz}{dy_{ijd''}}
%  x^{(2)}_{ijd},
%  \qquad
%  \frac{dz}{dx^{(2)}_{ijd}}
%   =
%  \frac{dz}{dy_{ijd}}
%  x^{(1)}_{ijd}.
%\]
%
%
%% ------------------------------------------------------------------
%\subsubsection{Split}\label{s:split}
%% ------------------------------------------------------------------
%
%\[
% y_{ijd}^{(1)} = x_{ijd}, \qquad y_{ijd}^{(2)} = x_{ijd}
%\]
%
%\[
% \frac{dz}{dx_{ijd}} =
%\sum_{i''j''d''}
% \frac{dz}{dy_{i''j''d''}^{(1)}}
%  \frac{dy_{i''j''d''}^{(1)}}{dx_{ijd}}
% +
%  \frac{dz}{dy_{i''j''d''}^{(2)}}
%  \frac{dy_{i''j''d''}^{(2)}}{dx_{ijd}}
%\]

% ------------------------------------------------------------------
\section{Pattern Recognition Methods Differences}\label{s:patt}
% ------------------------------------------------------------------

\subsection{Image Classification}\label{s:patt-class}
Image classification is the process of assigning land cover classes to pixels. Image classification refers to the task of extracting information classes from a multiband raster image. The resulting raster from image classification can be used to create thematic maps. Depending on the interaction between the analyst and the computer during classification, there are two types of classification: supervised and unsupervised. The image classification plays an important role in environmental and socioeconomic applications. In order to improve the classification accuracy, scientists have laid path in developing the advanced classification techniques.
Image classification analyzes the numerical properties of various image features and organizes data into categories. Classification algorithms typically employ two phases of processing: training and testing. In the initial training phase, characteristic properties of typical image features are isolated and, based on these, a unique description of each classification category, i.e. training class, is created. In the subsequent testing phase, these feature-space partitions are used to classify image features. The description of training classes is an extremely important component of the classification process. In supervised classification, statistical processes (i.e. based on an a priori knowledge of probability distribution functions) or distribution-free processes can be used to extract class descriptors. Unsupervised classification relies on clustering algorithms to automatically segment the training data into prototype classes. In either case, the motivating criteria for constructing training classes are that they are:
\begin{enumerate}
	\item Independent, e.a change in the description of one training class should not change the value of another,
	\item Discriminatory, e.different image features should have significantly different descriptions, and
	\item Reliable, all image features within a training group should share the common definitive descriptions of that group.
\end{enumerate}

A convenient way of building a parametric description of this sort is via a feature vector v1,v2,………,vn where n is the number of attributes which describe each image feature and training class. This representation allows us to consider each image feature as occupying a point, and each training class as occupying a sub-space (i.e. a representative point surrounded by some spread, or deviation), within the n-dimensional classification space. Viewed as such, the classification problem is that of determining to which sub-space class each feature vector belongs.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\linewidth]{images/classification_detection_segmentaion_comparisons.jpeg}
	\caption{Image Classification,Object detection Semantic Segmentation.}
\end{figure}
\subsection{Object Detection}\label{s:patt-dtct}
The goal of object detection is to detect all instances of objects from a known
class, such as people, cars or faces in an image. Typically only a small number
of instances of the object are present in the image, but there is a very large
number of possible locations and scales at which they can occur and that need
to somehow be explored.
Each detection is reported with some form of pose information. This could
be as simple as the location of the object, a location and scale, or the extent
of the object defined in terms of a bounding box. In other situations the pose
information is more detailed and contains the parameters of a linear or non-linear
transformation. For example a face detector may compute the locations of the
eyes, nose and mouth, in addition to the bounding box of the face. An example
of a vehicle and person detection that specifies the locations of certain parts is shown in
Figure 1. The pose could also be defined by a three-dimensional transformation
specifying the location of the object relative to the camera.
Object detection systems construct a model for an object class from a set of
training examples. In the case of a fixed rigid object only one example may be
needed, but more generally multiple training examples are necessary to capture
certain aspects of class variability.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\linewidth]{images/object_det.jpeg}
	\caption{Object detection with bounding boxes.}
\end{figure}

Object detection methods fall into two major categories, generative and discriminative. The first consists of a probability model for the pose variability of the objects together with an appearance model: a probability model for the image appearance conditional on a given pose, together with a model for background, i.e. non-object images. The model parameters can be estimated from training data and the decisions are based on ratios of posterior probabilities. The second typically builds a classifier that can discriminate between images (or sub-images) containing the object and those not containing the object. The parameters of the classifier are selected to minimize mistakes on the training data, often with a regularization bias to avoid overfitting. Other distinctions among detection algorithms have to do with the computational tools used to scan the entire image or search over possible poses, the type of image representation with which the models are constructed, and what type and how much training data is required to build a model.	

%\clearpage




\subsection{Semantic Segmentation}\label{s:patt-sema}
Segmentation is essential for image analysis tasks. Semantic segmentation describes the process of associating each pixel of an image with a class label, (such as flower, person, road, sky, ocean, or car).
Semantic image segmentation can be applied effectively to any task that involves the segmentation of visual information. Examples include road segmentation for autonomous vehicles, medical image segmentation, scene segmentation for robot perception, and in image editing tools. Whilst currently available systems provide accurate object recognition, they are unable to delineate the boundaries between objects with the same accuracy.

Oxford researchers have developed a novel neural network component for semantic segmentation that enhances the ability to recognise and delineate objects. This invention can be applied to improve any situation requiring the segmentation of visual information.

Semantic image segmentation plays a crucial role in image understanding, allowing a computer to recognise objects in images. Recognition and delineation of objects is achieved through classification of each pixel in an image. Such processes have a wide range of applications in computer vision, in diverse and growing fields such as vehicle autonomy and medical imaging.

The previous state-of-the-art image segmentation systems used Fully Convolutional Neural Network (FCNN) components, which offer excellent accuracy in recognising objects. Whilst this development represented a significant improvement in semantic segmentation, these networks do not perform well in delineating object boundaries. Conditional Random Fields (CRFs) can be employed in a post-processing step to improve object boundary delineation, however, this is not an optimum solution owing to a lack of integration with the deep network.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\linewidth]{images/semantic.jpg}
	\caption{Image with semantic segmentation.}
\end{figure}
Oxford researchers have developed a neural network component for semantic segmentation that harnesses the exceptional object recognition of FCNNs and the powerful boundary delineation of CRFs. CRFs are fully integrated as recurrent neural networks, resulting in a system that offers enhanced performance compared to the previous state-of-the-art. The novel system can be applied to any task that involves the segmentation of visual information. Examples include road segmentation for autonomous vehicles, medical image segmentation, scene segmentation for robot perception, and in image editing tools. Oxford University Innovation is seeking industrial partners that wish to explore the use of this system for commercial applications.
\subsubsection{Instance Segmentation}\label{s:patt-insta}
Instance segmentation is one step ahead of semantic segmentation wherein along with pixel level classification, we expect the computer to classify each instance of a class separately. For example in the image above there are 3 people, technically 3 instances of the class “Person”. All the 3 are classified separately (in a different color). But semantic segmentation does not differentiate between the instances of a particular class.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\linewidth]{images/instance.png}
	\caption{Image with instance segmentation.}
\end{figure}

% ------------------------------------------------------------------
\section{Implementation details}\label{s:impl}
% ------------------------------------------------------------------

This chapter contains calculations and details.

% ------------------------------------------------------------------
\subsection{Convolution}\label{s:impl-convolution}
% ------------------------------------------------------------------

It is often convenient to express the convolution operation in matrix form. To this end, let $\phi(\bx)$ be the \verb!im2row! operator, extracting all $W' \times H'$ patches from the map $\bx$ and storing them as rows of a $(H''W'') \times (H'W'D)$ matrix. Formally, this operator is given by:
\[
[\phi(\bx)]_{pq} \underset{(i,j,d)=t(p,q)}{=} x_{ijd}
\]
where the correspondence between indexes $(i,j,d)$ and $(p,q)$ is given by the map $(i,j,d) = t(p,q)$ where:
\[
i = i''+i'-1, \quad
j = j''+j'-1, \quad
p = i'' + H'' (j''-1), \quad
q = i' + H'(j'-1) + H'W' (d-1).
\]
In practice, this map is slightly modified to account for the padding, stride, and dilation factors. It is also useful to define the ``transposed'' operator \verb!row2im!:
\[
[\phi^*(M)]_{ijd}
=
\sum_{(p,q) \in t^{-1}(i,j,d)}
M_{pq}.
\]
Note that $\phi$ and $\phi^*$ are linear operators. Both can be expressed by a matrix $H\in\real^{(H''W''H'W'D) \times(HWD)}$ such that
\[
\vv(\phi(\bx)) = H \vv(\bx), \qquad 
\vv(\phi^*(M)) = H^\top \vv(M).
\]
Hence we obtain the following expression for the vectorized output (see~\cite{kinghorn96integrals}):
\[
\vv\by = 
\vv\left(\phi(\bx) F\right)
=
\begin{cases}
(I \otimes \phi(\bx)) \vv F, & \text{or, equivalently,} \\
(F^\top \otimes I) \vv \phi(\bx),
\end{cases}
\]
where $F\in\mathbb{R}^{(H'W'D)\times K}$ is the matrix obtained by reshaping the array $\bff$ and $I$ is an identity matrix of suitable dimensions. This allows obtaining the following formulas for the derivatives:
\[
\frac{dz}{d(\vv F)^\top}
=
\frac{dz}{d(\vv\by)^\top}
(I \otimes \phi(\bx))
= \vv\left[ 
\phi(\bx)^\top 
\frac{dz}{dY}
\right]^\top
\]
where $Y\in\real^{(H''W'')\times K}$ is the matrix obtained by reshaping the array $\by$. Likewise:
\[
\frac{dz}{d(\vv \bx)^\top}
=
\frac{dz}{d(\vv\by)^\top}
(F^\top \otimes I)
\frac{d\vv \phi(\bx)}{d(\vv \bx)^\top}
=
\vv\left[ 
\frac{dz}{dY}
F^\top
\right]^\top
H
\]
In summary, after reshaping these terms we obtain the formulas:
\[
\boxed{
	\vv\by = 
	\vv\left(\phi(\bx) F\right),
	\qquad
	\frac{dz}{dF}
	=
	\phi(\bx)^\top\frac{d z}{d Y},
	\qquad
	\frac{d z}{d X}
	=
	\phi^*\left(
	\frac{d z}{d Y}F^\top
	\right)
}
\]
where $X\in\real^{(HW)\times D}$ is the matrix obtained by reshaping $\bx$. Notably, these expressions are used to implement the convolutional operator; while this may seem inefficient, it is instead a fast approach when the number of filters is large and it allows leveraging fast BLAS and GPU BLAS implementations.

% ------------------------------------------------------------------
\subsection{Convolution transpose}\label{s:impl-convolution-transpose}
% ------------------------------------------------------------------

In order to understand the definition of convolution transpose, let $\by$ to be obtained from $\bx$ by the convolution operator as defined in \cref{s:convolution} (including padding and downsampling).  Since this is a linear operation, it can be rewritten as $\vv \by = M \vv\bx$ for a suitable matrix $M$; convolution transpose computes instead $\vv \bx = M^\top \vv \by$.  While this is simple to describe in term of matrices, what happens in term of indexes is tricky. In order to derive a formula for the convolution transpose, start from standard convolution (for a 1D signal):
\[
y_{i''} = \sum_{i'=1}^{H'} f_{i'} x_{S (i''-1) + i' - P_h^-}, 
\quad
1 \leq i'' \leq 1 + \left\lfloor \frac{H - H' + P_h^- + P_h^+}{S} \right\rfloor,
\]
where $S$ is the downsampling factor, $P_h^-$ and $P_h^+$ the padding, $H$ the length of the input signal $\bx$ and $H'$ the length of the filter $\bff$. Due to padding, the index of the input data $\bx$ may exceed the range $[1,H]$; we implicitly assume that the signal is zero padded outside this range.

In order to derive an expression of the convolution transpose,  we make use of the identity $\vv \by^\top (M \vv \bx) = (\vv \by^\top M) \vv\bx = \vv\bx^\top (M^\top \vv\by)$. Expanding this in formulas:
\begin{align*}
\sum_{i''=1}^b y_{i''} 
\sum_{i'=1}^{W'} f_{i'} x_{S (i''-1) + i'  -P_h^-}
&=
\sum_{i''=-\infty}^{+\infty}
\sum_{i'=-\infty}^{+\infty} 
y_{i''}\ f_{i'}\ x_{S (i''-1) + i'  -P_h^-}
\\
&=
\sum_{i''=-\infty}^{+\infty}
\sum_{k=-\infty}^{+\infty} 
y_{i''}\ f_{k-S(i'' -1) + P_h^-}\ x_{k}
\\
&=
\sum_{i''=-\infty}^{+\infty}
\sum_{k=-\infty}^{+\infty} 
y_{i''}%
\ %
f_{%
	(k-1+ P_h^-) \bmod S +
	S \left(1 -i''  + \left\lfloor \frac{k-1+ P_h^-}{S} \right\rfloor\right)+1
}\ x_{k}
\\
&=
\sum_{k=-\infty}^{+\infty} 
x_{k}
\sum_{q=-\infty}^{+\infty}
y_{\left\lfloor \frac{k-1+ P_h^-}{S} \right\rfloor + 2 - q}
\ %
f_{(k-1+ P_h^-)\bmod S +S(q - 1)+1}.
\end{align*}
Summation ranges have been extended to infinity by assuming that all signals are zero padded as needed. In order to recover such ranges, note that $k \in [1,H]$ (since this is the range of elements of $\bx$ involved in the original convolution). Furthermore, $q\geq 1$ is the minimum value of $q$ for which the filter $\bff$ is non zero; likewise, $q\leq \lfloor (H'-1)/S\rfloor +1$ is a fairly tight upper bound on the maximum value (although, depending on $k$, there could be an element less). Hence
\begin{equation}\label{e:convt-step}
x_k = 
\sum_{q=1}^{1 + \lfloor \frac{H'-1}{S} \rfloor}
y_{\left\lfloor \frac{k-1+ P_h^-}{S} \right\rfloor + 2 - q}\ %
f_{(k-1+ P_h^-)\bmod S +S(q - 1)+1},
\qquad k=1,\dots, H.
\end{equation}
Note that the summation extrema in \eqref{e:convt-step} can be refined slightly to account for the finite size of $\by$ and $\bw$:
\begin{multline*}
\max\left\{
1, 
\left\lfloor \frac{k-1 + P_h^-}{S} \right\rfloor + 2 - H''
\right\}
\leq q \\
\leq
1 +\min\left\{
\left\lfloor \frac{H'-1-(k-1+ P_h^-)\bmod S}{S} \right\rfloor, 
\left\lfloor \frac{k-1 + P_h^-}{S} \right\rfloor
\right\}.
\end{multline*}
The size $H''$ of the output of convolution transpose is obtained in \cref{s:receptive-convolution-transpose}.

% ------------------------------------------------------------------
\subsection{Spatial pooling}\label{s:impl-pooling}
% ------------------------------------------------------------------

Since max pooling simply selects for each output element an input element, the relation can be expressed in matrix form as
$
\vv\by = S(\bx) \vv \bx
$
for a suitable selector matrix $S(\bx)\in\{0,1\}^{(H''W''D) \times (HWD)}$. The derivatives can be written as:
$
\frac{d z}{d (\vv \bx)^\top}
=
\frac{d z}{d (\vv \by)^\top}
S(\bx),
$
for all but a null set of points, where the operator is not differentiable (this usually does not pose problems in optimization by stochastic gradient). For average pooling, similar relations exists with two differences: $S$ does not depend on the input $\bx$ and it is not binary, in order to account for the normalization factors. In summary, we have the expressions:
\begin{equation}\label{e:max-mat}
\boxed{
	\vv\by = S(\bx) \vv \bx,
	\qquad
	\frac{d z}{d \vv \bx}
	=
	S(\bx)^\top
	\frac{d z}{d \vv \by}.
}
\end{equation}



% ------------------------------------------------------------------
\subsection{Activation functions}\label{s:impl-activation}
% ------------------------------------------------------------------

% ------------------------------------------------------------------
\subsubsection{ReLU}\label{s:impl-relu}
% ------------------------------------------------------------------

The ReLU operator can be expressed in matrix notation as
\[
\vv\by = \diag\bfs \vv \bx,
\qquad
\frac{d z}{d \vv \bx}
=
\diag\bfs
\frac{d z}{d \vv \by}
\]
where $\bfs = [\vv \bx > 0] \in\{0,1\}^{HWD}$ is an indicator vector.

% ------------------------------------------------------------------
\subsubsection{Sigmoid}\label{s:impl-sigmoid}
% ------------------------------------------------------------------

The derivative of the sigmoid function is given by
\begin{align*}
\frac{dz}{dx_{ijd}}
&= 
\frac{dz}{d y_{ijd}} 
\frac{d y_{ijd}}{d x_{ijd}}
=
\frac{dz}{d y_{ijd}} 
\frac{-1}{(1+e^{-x_{ijd}})^2} ( - e^{-x_{ijd}})
\\
&=
\frac{dz}{d y_{ijd}} 
y_{ijd} (1 - y_{ijd}).
\end{align*}
In matrix notation:
\[
\frac{dz}{d\bx} = \frac{dz}{d\by} \odot 
\by \odot 
(\bone\bone^\top - \by).
\]


% ------------------------------------------------------------------
\subsection{Spatial bilinear resampling}\label{s:impl-sampler}
% ------------------------------------------------------------------

The projected derivative $d\langle \bp, \phi(\bx,\bg)\rangle / d\bx$ of the spatial bilinaer resampler operator with respect to the input image $\bx$ can be found as follows:
\begin{multline}\label{e:bilinear-back-x}
\frac{\partial}{\partial x_{ijc}}
\left[
\sum_{i''j''c''}
p_{i''k''c''}
\sum_{i'=1}^H
\sum_{j'=1}^W 
x_{i'j'c''}
\max\{0, 1-|\alpha_v g_{1i''j''} + \beta_v -i'|\}
\max\{0, 1-|\alpha_u g_{2i''j''} + \beta_u -j'|\}
\right]
\\
=
\sum_{i''j''}
p_{i''k''c}
\max\{0, 1-|\alpha_v g_{1i''j''} + \beta_v -i|\}
\max\{0, 1-|\alpha_u g_{2i''j''} + \beta_u -j|\}.
\end{multline}
Note that the formula is similar to Eq.~\ref{e:bilinear}, with the difference that summation is on $i''$ rather than $i$.

The projected derivative $d\langle \bp, \phi(\bx,\bg)\rangle / d\bg$ with respect to the grid is similar:
\begin{multline}\label{e:bilinear-back-g}
\frac{\partial}{\partial g_{1i'j'}}
\left[
\sum_{i''j''c}
p_{i''k''c}
\sum_{i=1}^H
\sum_{j=1}^W 
x_{ijc}
\max\{0, 1-|\alpha_v g_{1i''j''} + \beta_v -i|\}
\max\{0, 1-|\alpha_u g_{2i''j''} + \beta_u -j|\}
\right]
\\
=
-
\sum_c
p_{i'j'c}
\sum_{i=1}^H
\sum_{j=1}^W
\alpha_v x_{ijc}
\max\{0, 1-|\alpha_v g_{2i'j'} + \beta_v -j|\}
\sign(\alpha_v g_{1i'j'} + \beta_v -j)
\mathbf{1}_{\{-1 < \alpha_u g_{2i'j'} + \beta_u < 1\}}.
\end{multline}
A similar expression holds for $\partial g_{2i'j'}$

% ------------------------------------------------------------------
\subsection{Normalization}\label{s:normalization}
% ------------------------------------------------------------------

% ------------------------------------------------------------------
\subsubsection{Local response normalization (LRN)}\label{s:impl-ccnormalization}
% ------------------------------------------------------------------

The derivative is easily computed as:
\[
\frac{dz}{d x_{ijd}}
=
\frac{dz}{d y_{ijd}}
L(i,j,d|\bx)^{-\beta}
-2\alpha\beta x_{ijd}
\sum_{k:d\in G(k)}
\frac{dz}{d y_{ijk}}
L(i,j,k|\bx)^{-\beta-1} x_{ijk} 
\]
where
\[
L(i,j,k|\bx) = \kappa + \alpha \sum_{t\in G(k)} x_{ijt}^2.
\]

% ------------------------------------------------------------------
\subsubsection{Batch normalization}\label{s:impl-bnorm}
% ------------------------------------------------------------------

The derivative of the network output $z$ with respect to the multipliers $w_k$ and biases $b_k$ is given by
\begin{align*}
\frac{dz}{dw_k} &= \sum_{i''j''k''t''}
\frac{dz}{d y_{i''j''k''t''}} 
\frac{d y_{i''j''k''t''}}{d w_k}
=
\sum_{i''j''t''}
\frac{dz}{d y_{i''j''kt''}} 
\frac{x_{i''j''kt''} - \mu_{k}}{\sqrt{\sigma_k^2 + \epsilon}},
\\
\frac{dz}{db_k} &= \sum_{i''j''k''t''}
\frac{dz}{d y_{i''j''k''t''}} 
\frac{d y_{i''j''k''t''}}{d w_k}
=
\sum_{i''j''t''}
\frac{dz}{d y_{i''j''kt''}}.
\end{align*}

The derivative of the network output $z$ with respect to the block input $x$ is computed as follows:
\[
\frac{dz}{dx_{ijkt}} = \sum_{i''j''k''t''}
\frac{dz}{d y_{i''j''k''t''}} 
\frac{d y_{i''j''k''t''}}{d x_{ijkt}}.
\]
Since feature channels are processed independently, all terms with $k''\not=k$ are zero. Hence
\[
\frac{dz}{dx_{ijkt}} = \sum_{i''j''t''}
\frac{dz}{d y_{i''j''kt''}} 
\frac{d y_{i''j''kt''}}{d x_{ijkt}},
\]
where
\[
\frac{d y_{i''j''kt''}}{d x_{ijkt}} 
=
w_k
\left(\delta_{i=i'',j=j'',t=t''} - \frac{d \mu_k}{d x_{ijkt}}\right)
\frac{1}{\sqrt{\sigma^2_k + \epsilon}}
-
\frac{w_k}{2}
\left(x_{i''j''kt''} - \mu_k\right)
\left(\sigma_k^2 + \epsilon \right)^{-\frac{3}{2}}
\frac{d \sigma_k^2}{d x_{ijkt}},
\]
the derivatives with respect to the mean and variance are computed as follows:
\begin{align*}
\frac{d \mu_k}{d x_{ijkt}} &= \frac{1}{HWT},
\\
\frac{d \sigma_k^2}{d x_{i'j'kt'}}
&=
\frac{2}{HWT}
\sum_{ijt}
\left(x_{ijkt} - \mu_k \right)
\left(\delta_{i=i',j=j',t=t'} - \frac{1}{HWT} \right)
=
\frac{2}{HWT} \left(x_{i'j'kt'} - \mu_k \right),
\end{align*}
and $\delta_E$ is the indicator function of the event $E$. Hence
\begin{align*}
\frac{dz}{dx_{ijkt}}
&=
\frac{w_k}{\sqrt{\sigma^2_k + \epsilon}}
\left(
\frac{dz}{d y_{ijkt}} 
-
\frac{1}{HWT}\sum_{i''j''kt''}
\frac{dz}{d y_{i''j''kt''}} 
\right)
\\
&-
\frac{w_k}{2(\sigma^2_k + \epsilon)^{\frac{3}{2}}}
\sum_{i''j''kt''}
\frac{dz}{d y_{i''j''kt''}} 
\left(x_{i''j''kt''} - \mu_k\right)
\frac{2}{HWT} \left(x_{ijkt} - \mu_k \right)
\end{align*}
i.e.
\begin{align*}
\frac{dz}{dx_{ijkt}}
&=
\frac{w_k}{\sqrt{\sigma^2_k + \epsilon}}
\left(
\frac{dz}{d y_{ijkt}} 
-
\frac{1}{HWT}\sum_{i''j''kt''}
\frac{dz}{d y_{i''j''kt''}} 
\right)
\\
&-
\frac{w_k}{\sqrt{\sigma^2_k + \epsilon}}
\,
\frac{x_{ijkt} - \mu_k}{\sqrt{\sigma^2_k + \epsilon}}
\,
\frac{1}{HWT}
\sum_{i''j''kt''}
\frac{dz}{d y_{i''j''kt''}} 
\frac{x_{i''j''kt''} - \mu_k}{\sqrt{\sigma^2_k + \epsilon}}.
\end{align*}
We can identify some of these terms with the ones computed as derivatives of bnorm with respect to $w_k$ and $\mu_k$:
\begin{align*}
\frac{dz}{dx_{ijkt}}
&=
\frac{w_k}{\sqrt{\sigma^2_k + \epsilon}}
\left(
\frac{dz}{d y_{ijkt}} 
-
\frac{1}{HWT}
\frac{dz}{d b_k} 
-
\frac{x_{ijkt} - \mu_k}{\sqrt{\sigma^2_k + \epsilon}}
\,
\frac{1}{HWT}
\frac{dz}{dw_k}
\right).
\end{align*}

% ------------------------------------------------------------------
\subsubsection{Spatial normalization}\label{s:impl-spnorm}
% ------------------------------------------------------------------

The neighbourhood norm $n^2_{i''j''d}$ can be computed by applying average pooling to $x_{ijd}^2$ using \verb!vl_nnpool! with a $W'\times H'$ pooling region, top padding $\lfloor \frac{H'-1}{2}\rfloor$, bottom padding $H'-\lfloor \frac{H-1}{2}\rfloor-1$, and similarly for the horizontal padding.

The derivative of spatial normalization can be obtained as follows:
\begin{align*}
\frac{dz}{dx_{ijd}} 
&= \sum_{i''j''}
\frac{dz}{d y_{i''j''d}} 
\frac{d y_{i''j''d}}{d x_{ijd}}
\\
&=
\sum_{i''j''}
\frac{dz}{d y_{i''j''d}} 
(1 + \alpha n_{i''j''d}^2)^{-\beta}
\frac{dx_{i''j''d}}{d x_{ijd}} 
-\alpha\beta
\frac{dz}{d y_{i''j''d}} 
(1 + \alpha n_{i''j''d}^2)^{-\beta-1}
x_{i''j''d}
\frac{dn_{i''j''d}^2}{d (x^2_{ijd})} 
\frac{dx^2_{ijd}}{d x_{ijd}}
\\
&=
\frac{dz}{d y_{ijd}} 
(1 + \alpha n_{ijd}^2)^{-\beta}
-2\alpha\beta x_{ijd}
\left[
\sum_{i''j''}
\frac{dz}{d y_{i''j''d}} 
(1 + \alpha n_{i''j''d}^2)^{-\beta-1}
x_{i''j''d}
\frac{dn_{i''j''d}^2}{d (x_{ijd}^2)}
\right]
\\
&=
\frac{dz}{d y_{ijd}} 
(1 + \alpha n_{ijd}^2)^{-\beta}
-2\alpha\beta x_{ijd}
\left[
\sum_{i''j''}
\eta_{i''j''d}
\frac{dn_{i''j''d}^2}{d (x_{ijd}^2)}
\right],
\quad
\eta_{i''j''d}=
\frac{dz}{d y_{i''j''d}} 
(1 + \alpha n_{i''j''d}^2)^{-\beta-1}
x_{i''j''d}
\end{align*}
Note that the summation can be computed as the derivative of the
\verb!vl_nnpool! block.

% ------------------------------------------------------------------
\subsubsection{Softmax}\label{s:impl-softmax}
% ------------------------------------------------------------------

Care must be taken in evaluating the exponential in order to avoid underflow or overflow. The simplest way to do so is to divide the numerator and denominator by the exponential of the maximum value:
\[
y_{ijk} = \frac{e^{x_{ijk} - \max_d x_{ijd}}}{\sum_{t=1}^D e^{x_{ijt}- \max_d x_{ijd}}}.
\]
The derivative is given by:
\[
\frac{dz}{d x_{ijd}}
=
\sum_{k}
\frac{dz}{d y_{ijk}}
\left(
e^{x_{ijd}} L(\bx)^{-1} \delta_{\{k=d\}}
-
e^{x_{ijd}}
e^{x_{ijk}} L(\bx)^{-2}
\right),
\quad
L(\bx) = \sum_{t=1}^D e^{x_{ijt}}.
\]
Simplifying:
\[
\frac{dz}{d x_{ijd}}
=
y_{ijd} 
\left(
\frac{dz}{d y_{ijd}}
-
\sum_{k=1}^K
\frac{dz}{d y_{ijk}} y_{ijk}
\right).
\]
In matrix form:
\[
\frac{dz}{dX} = Y \odot \left(\frac{dz}{dY} 
- \left(\frac{dz}{dY} \odot Y\right) \bone\bone^\top\right)
\]
where $X,Y\in\real^{HW\times D}$ are the matrices obtained by reshaping the arrays
$\bx$ and $\by$. Note that the numerical implementation of this expression is straightforward once the output $Y$ has been computed with the caveats above.

% ------------------------------------------------------------------
\subsection{Categorical losses}\label{s:impl-losses}
% ------------------------------------------------------------------

This section obtains the projected derivatives of the categorical losses in \cref{s:losses}. Recall that all losses give a scalar output, so the projection tensor $p$ is trivial (a scalar).

% ------------------------------------------------------------------
\subsubsection{Classification losses}\label{s:impl-loss-classification}
% ------------------------------------------------------------------

\paragraph{Top-$K$ classification error.} The derivative is zero a.e.\

\paragraph{Log-loss.} The projected derivative is:
\[
\frac{\partial p \ell(\bx,c)}{\partial x_k}
=
- p \frac{\partial \log (x_c) }{\partial x_k}
=
- p x_c \delta_{k=c}.
\]

\paragraph{Softmax log-loss.} The projected derivative is given by:
\[
\frac{\partial p \ell(\bx,c)}{\partial x_k}
=
- p \frac{\partial}{\partial x_k}
\left(x_c - \log \sum_{t=1}^C e^{x_t}\right)
=
- p \left(\delta_{k=c} - \frac{e^{x_c}}{\sum_{t=1}^C e^{x_t}} \right).
\]
In brackets, we can recognize the output of the loss itself:
\[
y = \ell(\bx,c) = \frac{e^{x_c}}{\sum_{t=1}^C e^{x_t}}.
\]
Hence the loss derivatives rewrites:
\[
\frac{\partial p \ell(\bx,c)}{\partial x_k}
=
- p \left(\delta_{k=c} - y \right).
\]

\paragraph{Multi-class hinge loss.} The projected derivative is:
\[
\frac{\partial p \ell(\bx,c)}{\partial x_k}
=
- p\,\mathbf{1}[x_c < 1]\,\delta_{k=c}.
\]

\paragraph{Structured multi-class hinge loss.} The projected derivative is:
\[
\frac{\partial p \ell(\bx,c)}{\partial x_k}
=
- p\,\mathbf{1}[x_c < 1 + \max_{t\not= c} x_t]\,(\delta_{k=c} - \delta_{k=t^*}),
\qquad
t^* = \argmax_{t =1,2,\dots,C} x_t.
\]

% ------------------------------------------------------------------
\subsubsection{Attribute losses}\label{s:impl-loss-attribute}
% ------------------------------------------------------------------

\paragraph{Binary error.} The derivative of the binary error is 0 a.e.

\paragraph{Binary log-loss.} The projected derivative is:
\[
\frac{\partial p \ell(x,c)}{\partial x}
=
- p \frac{c}{c \left(x - \frac{1}{2}\right) + \frac{1}{2}}.
\]

\paragraph{Binary logistic loss.} The projected derivative is:
\[
\frac{\partial p \ell(x,c)}{\partial x}
=
- p \frac{\partial}{\partial x} \log \frac{1}{1+e^{-cx}}
=
- p \frac{c e^{-cx}}{1 + e^{-cx}}
=
- p \frac{c}{e^{cx} + 1}
=
- pc\, \sigma(-cx).
\]

\paragraph{Binary hinge loss.} The projected derivative is
\[
\frac{\partial p \ell(x,c)}{\partial x}
=
- pc\,\mathbf{1}[cx < 1].
\]

% ------------------------------------------------------------------
\subsection{Comparisons}\label{s:impl-comparisons}
% ------------------------------------------------------------------

% ------------------------------------------------------------------
\subsubsection{$p$-distance}\label{s:impl-pdistance}
% ------------------------------------------------------------------

The derivative of the operator without root is given by:
\begin{align*}
\frac{dz}{dx_{ijd}}
&=
\frac{dz}{dy_{ij}}
p |x_{ijd} - \bar x_{ijd}|^{p-1} \operatorname{sign} (x_{ijd} - \bar x_{ijd}).
\end{align*}
The derivative of the operator with root is given by:
\begin{align*}
\frac{dz}{dx_{ijd}}
&=
\frac{dz}{dy_{ij}}
\frac{1}{p}
\left(\sum_{d'} |x_{ijd'} - \bar x_{ijd'}|^p \right)^{\frac{1}{p}-1}
p |x_{ijd} - \bar x_{ijd}|^{p-1} \sign(x_{ijd} - \bar x_{ijd})
\\
&= 
\frac{dz}{dy_{ij}}
\frac{|x_{ijd} - \bar x_{ijd}|^{p-1} \sign(x_{ijd} - \bar x_{ijd})}{y_{ij}^{p-1}}, \\
\frac{dz}{d\bar x_{ijd}} &= -\frac{dz}{dx_{ijd}}.
\end{align*}
The formulas simplify a little for $p=1,2$ which are therefore implemented as special cases.


% ------------------------------------------------------------------
\subsection{Other implementation details}\label{s:impl-others}
% ------------------------------------------------------------------

% ------------------------------------------------------------------
\subsubsection{Normal sampler}\label{s:impl-normal}
% ------------------------------------------------------------------

The function \verb!vl::randn()! uses the Ziggurah method~\cite{marsaglia00the-ziggurat} to sample from a Normally-distributed random variable. Let $f(x) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{1}{2}x^2\right)$ the standard Normal distribution. The sampler encloses $f(x)$ in a simple shape made of $K-1$ horizontal rectangles and a base composed of a rectangle tapering off in an exponential distribution. These are defined by points $x_1 > x_2 > x_3 > \dots > x_K=0$ such that (for the right half of $f(x)$) the layers of the Ziggurat are given by
\[
\forall k=1,\dots,K-1:
\quad R_k = [f(x_k),f(x_{k+1})] \times [0,x_k].
\]
and such that its basis is given by
\[
R_0 = ([0,f(x_{1})] \times [0,x_1]) \cup 
\{ (x,y) : x \geq x_1,\ y \leq f(x_1) \exp(-x_1(x - x_1)) \}
\]
Note that, since the last point $x_K=0$, (half of) the distribution is enclosed by the Ziggurat, i.e. $\forall x \geq 0:(x,f(x)) \in \cup_{k=0}^K R_k$.

The first point $x_1$ in the sequence determines the area of the Ziggurat base:
\[
A = |R_0| = f(x_1)x_1 + f(x_1)/x_1.
\]
The other points are defined recursively such that the area is the same for all rectangles:
\[
A = |R_k| = (f(x_{k+1}) - f(x_k))x_k
\quad\Rightarrow\quad
x_{k+1} = f^{-1} (A/x_k + f(x_k)).
\]
There are two degrees of freedom: the number of subdivisions $K$ and the point $x_1$. Given $K$, the goal is to choose $x_1$ such that the $K$-th points $x_K=0$ lands on zero, enclosing tightly $f(x)$. The required value of $x_1$ is easily found using bisection and, for $K=256$, is $x_1=3.655420419026953$. Given $x_1$, $A$ and all other points in the sequence can be derived easily using the formulas above.

The Ziggurat can be used to quickly sample from the Normal distribution. In order to do so, one first samples a point $(x,y)$ uniformly at random from the Ziggurat $\cup_{k=0}^K R_k$ and then rejects pairs $(x,y)$ that do not belong to the graph of $f(x)$, i.e.\ $y > f(x)$. Specifically:
\begin{enumerate}
	\item Sample a point $(x,y)$ uniformly from the Ziggurat. To do so, sample uniformly at random an index $k \in\{0,1,\dots,K-1\}$ and two scalars $u,v$ in the interval $[0,1)$. Then, for $k\geq 1$, set $x = u x_k$ and $y = v f(x_{k+1}) + (1-v)f(x_k)$ (for $k=0$ see below). Since all regions $R_k$ have the same area and $(x,y)$ are then drawn uniformly form the selected rectangle, this samples a point $(x,y)$ from the Ziggurat uniformly at random.
	\item If $y \leq f(x)$, accept $x$ as a sample; otherwise, sample again. Note that, when $x \leq x_{k+1}$, the test $y \leq f(x_{k+1}) < f(x)$ is always successful, and the variable $y$ and test can be skipped in the step above.
\end{enumerate}
Next, we complete the procedure for $k=0$, when $R_0$ is not just a rectangle but rather the union of a rectangle and an exponential distribution. To sample from $R_0$ uniformly, we either choose the rectangle or the exponential distribution with a probability proportional to their area. Reusing the notation (and corresponding code) above, we can express this as sampling $x = u x_0$ and accepting the latter as a sample from the rectangle component if $ux_0 \leq x_1$; here the pseudo-point $x_0$ is defined such that $x_1 / x_0 = f(x_1)x_1 / A$, i.e.\ $x_0 = A/f(x_1)$. If the test fails, we sample instead from the exponential distribution $x\sim x_1\exp(-x_1(x-x_1)),$ $x\geq x_1$. To do so, let $z= x_1\exp(-x_1(x-x_1))$; then $x = x_1 - (1/x_1) \ln z/x_1$ and $dx = |- (x_1/z)|dz$, where $z\in(0,x_1]$. Since $x_1\exp(-x_1(x-x_1)) dx = (1/x_1) dz$ is uniform, we can implement this by sampling $u$ uniformly in $(0,1]$ and setting $x = x_1 - (1/x_1) \ln u$. Finally, recall that the goal is to sample from the Normal distribution, not the exponential, so the latter sample must be refined by rejection sampling. As before, this requires sampling a pair $(x,y)$ under the exponential distribution graph. Given $x$ sampled from the exponential distribution, we sample the corresponding $y$ uniformly at random in the interval $[0, f(x_1) \exp(-x_1(x-x_1))]$, and write the latter as $y = v f(x_1) \exp(-x_1(x-x_1))$, where $v$ is uniform in $[0,1]$. The latter is then accepted provided that $y$ is below the Normal distribution graph $f(x)$, i.e. $v f(x_1) \exp(-x_1(x-x_1)) \leq f(x).$ A short calculation yields the test:
\[
-2\ln v \geq x_1^2 +  x^2 - 2x_1x = (x_1 - x)^2 =
((1/x_1) \ln u)^2.
\]

% ------------------------------------------------------------------
\subsubsection{Euclid's algorithm}\label{s:impl-euclid}
% ------------------------------------------------------------------

Euclid's algorithm finds the \emph{greatest common divisor} (GCD) of two non-negative integers $a$ and $b$. Recall that the GCD is the largest integer that divides both $a$ and $b$:
\[
\gcd(a,b) = \max\{ d \in \mathbb{N} : d|a \ \wedge\ d|b \}.
\]

\begin{lemma}[Euclid's algorithm]
	Let $a,b\in\mathbb{N}$ and let $q\in\mathbb{Z}$ such that $a - qb \geq 0$. Then
	$$
	\gcd(a,b) = \gcd(a-qb,b).
	$$
\end{lemma}
\begin{proof}
	Let $d$ be a divisor of both $a$ and $b$. Then $d$ divides $a - qb$ as well because:
	$$
	\frac{a - qb}{d} = 
	\underbrace{\frac{a}{d}}_{\in\mathcal{Z}} - q 
	\underbrace{\frac{b}{d}}_{\in\mathbb{Z}}
	\quad\Rightarrow\quad
	\frac{a - qb}{d} \in \mathbb{Z}.
	$$
	Hence  $\gcd(a,b) \leq \gcd(q-qb,b)$. In the same way, we can show that, if $d$ divides $a - qb$ as well as $b$, then it must divide $a$ too, hence $\gcd(a-qb, b) \leq\gcd(a,b)$.
\end{proof}

Euclid's algorithm starts with $a > b \geq 1$ and sets $q$ to the quotient of the integer division $a/b$. Due to the lemma above, the GCD of $a$ and $b$ is the same as the GCD of the remainder $r = a - qb = (a \mod b)$ and $b$:
\[
\gcd(a,b) = \gcd(a, a\mod b).
\]
Since the remainder $(a\mod b) < b$ is strictly smaller than $b$, now GCD is called with smaller arguments. The recursion terminates when a zero reminder is generated, because
\[
\gcd(a,0) = a.
\]

We can modify the algorithms to also find two integers  $u,v$, the B\'ezout's coefficients, such that:
$$
a u + bv = \gcd(a,b).
$$
To do so, we replace $a = b (a/b) + r$ as above:
$$
ru  +  b v'= \gcd(a,b) = \gcd(r,b), \qquad v' = \frac{a}{b} u + v.
$$
The recursion terminates when $r=0$, in which case
$$
b v'= \gcd(0,b) = b \quad\Rightarrow\quad v'=b.
$$





