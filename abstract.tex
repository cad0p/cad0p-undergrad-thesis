\chapter*{Abstract}
\markboth{Abstract}{Abstract}
%\addcontentsline{toc}{chapter}{Abstract}
\selectlanguage{english}
\section*{English Abstract}

The task I was given was detecting clothing and shoes in real world images -- the IMP Lab team of the University of Parma had previously only trained the algorithms using studio quality images.

Apparel instance segmentation is composed of two tasks: finding the objects (instances), and drawing a line around them (segmentation). What previously had to be done by hand, now can be done automatically by a computer. This work allows for future implementation on, for example, Instagram -- the biggest social site for apparel images. We could track how a clothing item is used, where, and with whom. Or measure how well a Creator/Influencer is doing in promoting the products he/she was paid to wear.

What I did was use an existing neural network and search for a dataset to train it for detecting clothing on user generated images. This means that the program (for which I created a Command Line Interface) takes as input an image file (or a URL to an image) and returns a processed image; in which annotations such as whether the person is wearing pants, skirts, a dress, footwear or other labels are appended and shown, with a rectangular bounding box and a corresponding mask inside.

The dataset I found is called \modanet, and it is really based on the Paperdoll dataset. \modanet (by eBay researchers) has put annotations to this enormous dataset (about 1 million images)[but only 55k are annotated].

So the work I've done is integrating the dataset with the training algorithm. Fortunately, the \modanet annotations already used COCO's style, but the images were in an SQL database I had to extract (and then put the extraction program into a one-click CLI process!).
Then, I had to create another program that split the annotations into training, validation and test and organize them correctly. The first few weeks were very disappointing and it felt like combining this particular dataset was an impossible feat. It turned out that the lab computer Keras bad installation was also at fault.
Really the first assignment I was given, which I solved relatively quickly, was, using the default image provided and the default weights of \maskrcnn, to cut the image into several segments, each one showing only one object (annotation).

As for the splitting datasets annotations task, I made two revisions. The first one was splitting by annotations but left some images in more than one set. So you could have had the same image on different sets but some annotations for that image were in one set and some in the other. This created confusion in the algorithm, but made the task of creating a heterogeneous and balanced training set easier. Second revision was the more classic (I then discovered) approach: putting an image in only one set, and with all its annotations. This made the task of keeping the sets balanced in terms of categories of labels on each set more difficult, but I made it!

After completing the first phase (i.e. the "making it work" one), I was advised to create a package for one-click install of the things I had done to make it work.

In the meantime, at dark times when I couldn't go forward without some advising, I let the computer do its things and run the training algorithms. The most I managed to keep it running was about 3-4 days (the computer was shared amongst other researchers and thesists). I managed to reach epoch 28, and I got quite nice results, even though it has a hard time recognizing both feet's footwear most of the time. 

I also made nice customizable viewing commands to see the processed images, and to quickly scan through the validation set (via the --set-all option) to see if the algorithm has improved or not based on your training.

I then fixed a longtime error -- many annotations gave a warning of “invalid indexes" during training because they went outside the images borders. This way, the dataset got twice as big in an instant, so I decided to redo all the tests. I also discovered that many footwear annotations had been wrongly labeled, with overlapping bounding boxes and shapes belonging to more than one box. There are still cases of that happening now, but they have been almost halved.

This allowed the algorithm to better grasp footwear and boots, something it had never been able to do in months.
\pagebreak

\selectlanguage{italian}
\section*{Italian Abstract}


Il task che ho scelto era di riconoscere vestiti e scarpe in immagini scattate in condizioni naturali -- l'IMP Lab dell'Università con cui ho svolto questa tesi aveva fino ad ora solo addestrato gli algoritmi usando immagini di alta qualità e con sfondo uniforme.

Il problema di segmentazione di istanze di abbigliamento è diviso in due parti: trovare gli oggetti (instances), e contornarli (segmentation). Quello che prima doveva essere fatto a mano, ora piò essere fatto da un computer. Questa implementazione permette futuri lavori di applicazione su, per esempio, Instagram -- il social network più in voga oggi per condivisione di capi di abbigliamento. Si potrebbe analizzare ad esempio come un tipo di vestito è usato, in che contesto e con chi. O misurare l'impatto che gli Influencers hanno sui propri fan, in relazione agli oggetti che promuovono (e che sono pagati per promuovere).

Quello che ho fatto è usare un neural network esistente e cercare un dataset per fargli imparare a riconoscere abbigliamento in immagini anche non professionali scattate da persone qualsiasi. Questo significa che il programma (per il quale ho creato una Command Line Interface) prende come input un file di immagine (o l'URL a un'immagine) e ritorna un'immagine processata; nella quale le annotazioni, come per esempio se una persona indossa pantaloni, gonne, un vestito o altri label, sono aggiunte all'immagine e mostrate all'utente. Con un bounding box rettangolare e una maschera corrispondente all'interno.

Il dataset che ho trovato si chiama \modanet, e si basa in realtà su Paperdoll.
\modanet (fatto da ricercatori di eBay) ha annotato più di 50mila immagini (anche se l'intero dataset consiste di quasi un milione di immagini).

Quindi il lavoro che ho fatto è stato integrare il dataset con l'algoritmo di training. Fortunatamente, le annotazioni \modanet seguivano già lo stile COCO, però le immagini erano compresse dentro a un database SQL che ho dovuto estrarre (e poi automatizzare questo processo nella CLI!). Successivamente, ho dovuto creare un altro programma per dividere le annotazioni in training, validation and test set, organizzandole correttamente.
Le prime settimane sono state davvero deludenti e sembrava che il dataset e il training proprio non volessero funzionare insieme. Alla fine abbiamo persino dovuto reinstallare Keras perchè era stato installato male! Il primo compito che mi era stato assegnato l'avevo in realtà risolto in fretta, ovvero dovevo usare un modello (già addestrato dai creatori di \maskrcnn per riconoscere alcuni oggetti in un'immagine) e invece che far vedere l'immagine intera annotata, far vedere soltanto le singole annotazioni una per una.

Per il programma di suddivione delle annotazioni, ho fatto due versioni. La prima le divideva per singole annotazioni, ma permetteva a un'immagine di appartenere a più dataset. Naturalmente con alcune annotazioni di quell'immagine in un set e altre nell'altro. La seconda (che ho poi scoperto essere il metodo classico) le suddivideva per immagini, quindi prendeva un'immagine e la metteva in un set, con tutte le sue annotazioni.

Dopo aver completato la prima fase, mi è stato suggerito di creare un package per permettere a chiunque di installare e rendere pronto all'uso il mio elaborato.
Nel frattempo che io cercavo di risolvere i problemi insormontabili, il computer lavorava per giorni per addestrare l'algoritmo sul dataset. Il massimo che sono riuscito a tenerlo a lavorare è 3-4 giorni (il PC è condiviso con altri tesisti). Sono riuscito a raggiungere l'epoca 28 e ho raggiunto ottimi risultati, nonostante qualche difficolta nel trovare scarpe e footwear.

Ho fatto anche utili comandi per vedere facilmente il risultato dando in pasto immagini manualmente o dal validation set.

Ho poi sistemato un errore che avevamo dato per perso: molte annotazioni venivano buttate via perchè uscivano, seppur di poco, dall'immagine. Dopo aver verificato che le annotazioni "invalide" erano corrette, le abbiamo ridimensionate per farle stare dentro all'immagine. Per questo avevo deciso di rifare i test per vedere i miglioramenti. I risultati dopo questo test erano quasi identici. Infatti il problema non era nelle annotazioni che andavano fuori dall'immagine, ma nei bounding box che erano sovrapposti e non seguivano le segmentazioni di scarpe e footwear. Ho quindi sistemato, per quanto possibile, le annotazioni e i bounding box sovrapposti sono stati dimezzati.

Grazie a quest'ultimo accorgimento, finalmente l'algoritmo è ora in grado di riconoscere correttamente tutte e 13 le categorie per cui è stato addestrato.



\selectlanguage{english}
